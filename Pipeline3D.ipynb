{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8aa67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d0bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b60c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d5be9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17734132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si se desea graficar\n",
    "#conda install -c conda-forge cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "655158d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43a709e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a38401a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbe66ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manejo de Datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Imagenes satelitales\n",
    "#import cartopy.crs as ccrs\n",
    "#import cartopy.feature as cf\n",
    "from netCDF4 import Dataset, num2date\n",
    "\n",
    "#Machine learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#Librerias estandar (Extras)\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45d432e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda uninstall cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ff0786b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10576170694722177446\n",
      "xla_global_id: -1\n",
      "]\n",
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a24f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#El modelo solo considera en input_shape(x,x,1), el 1 se puede cambiar para abarcar mas canales de imagenes satelitales\n",
    "def crearModelo(dimTime,W,H,dimCanal, output):\n",
    "    \n",
    "    print(f\"Se creo un modelo con input ({dimTime}, {W},{H}, {dimCanal}) y output({output})\")\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv3D(32, (3, 3,3), activation='relu', input_shape=(dimTime, W, H, dimCanal)))\n",
    "    model.add(layers.MaxPooling3D((2, 2,2)))\n",
    "    #model.add(layers.Conv3D(128, (3, 3,3), activation='relu'))\n",
    "    #model.add(layers.MaxPooling3D((2, 2,2)))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(output))\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d73482ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMatriz(filename, origen, margen):    \n",
    "    \n",
    "    try:\n",
    "        ds = Dataset(filename)\n",
    "    except:\n",
    "        print(\"No se pudo leer el archivo\")\n",
    "        print(filename)\n",
    "        return -1\n",
    "\n",
    "    #El ancho y alto sera el margen que se dara desde el punto de origen (estacion)\n",
    "    #Esta en decimales (1 decimal == 100Km) - (Temporal)\n",
    "    alto= margen[0]\n",
    "    ancho= margen[1]\n",
    "    \n",
    "    \n",
    "    #Obteine los datos de la imagen satelital\n",
    "    field = ds.variables['CMI'][:].data.astype(np.float32)/100.0      \n",
    "    lon = ds.variables['longitude'][:].data\n",
    "    lat = ds.variables['latitude'][:].data    \n",
    "\n",
    "    #Se define el margen para recortar la imagen satelital\n",
    "    maxLon=origen[0]+ancho\n",
    "    minLon=origen[0]-ancho\n",
    "    maxLat=origen[1]+alto\n",
    "    minLat=origen[1]-alto\n",
    "\n",
    "    #Booleanos que ayudarán a buscar el margen\n",
    "    altoMin = False\n",
    "    altoMax = False\n",
    "\n",
    "\n",
    "    #Inicializamos los \"indices\"\n",
    "    lom = 0\n",
    "    loM = 0\n",
    "    lam = 0\n",
    "    laM = 0\n",
    "\n",
    "    \"\"\"\n",
    "    Tener en cuenta que el arreglo de longitudes (lon) esta ordenado de manera creciente,\n",
    "    mientras que el de latitudes (lat) esta de manera decreciente\n",
    "    \"\"\"    \n",
    "    for i in range(0,len(lon)):\n",
    "        if lon[i]>=minLon and not altoMin:\n",
    "            altoMin = True\n",
    "            lom = i\n",
    "        if lon[i]<=maxLon:\n",
    "            loM = i\n",
    "\n",
    "    for j in range(0,len(lat)):\n",
    "        if lat[j]>=minLat:    \n",
    "            laM = j\n",
    "        if lat[j]<=maxLat and not altoMax:\n",
    "            altoMax = True\n",
    "            lam = j  \n",
    "            \n",
    "    return field[lam:laM,lom:loM]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05306124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se le da un tensor de 4 dimensiones\n",
    "#[0] =  dato de precipitacion\n",
    "#[1] = Punto de la estacion (Longitud)\n",
    "#[2] = Punto de la estacion (Latitud)\n",
    "#[3] = Fecha (año-mes-dia-hora)\n",
    "\n",
    "#Devuelve x,y\n",
    "#X = Dato de precipitacion\n",
    "#Y = np.Array de las matrices de colores de cada producto en products (C08,C07 o C13)\n",
    "def leerImagenArea(tensor, umbral, path_base,margen,products, imprimir=0): \n",
    "    \n",
    "    \"\"\"\n",
    "    -----------------------------------------------------------------------------------    \n",
    "    !!!!!VERIFICAR QUE LA HORA DE LA IMAGEN SATELITAL SEA IGUAL A LA HORA PERU!!!!!!!!\n",
    "    -----------------------------------------------------------------------------------\n",
    "    \n",
    "    Los archivos se deben encontrar en carpetas ordenadas : ../GOES/{producto}/{año}/{mes}/{ARCHIVO}.nc\n",
    "    ARCHIVO = G16_{producto}_Cyl_{año}{mes}{dia}-{hora}00.nc'\n",
    "    \n",
    "    EJEMPLO : path_base + GOES/C8/2019/02/G16_C08_Cyl_20190210-1600.nc\n",
    "    \"\"\"\n",
    "    \n",
    "    #Se define por defecto el path base - (Temporal)    \n",
    "    try:\n",
    "        #Fecha = 2019 01 05 22\n",
    "        fecha = str(tensor.numpy()[3].decode('UTF-8'))\n",
    "        year,month,day,hour = fecha.split('-')        \n",
    "   \n",
    "    except:\n",
    "        print(\"No se pudo leer la fecha\")\n",
    "        print(tensor.numpy()[3].decode('UTF-8'))\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "    origen = [float(tensor.numpy()[1].decode('UTF-8')),float(tensor.numpy()[2].decode('UTF-8'))] \n",
    "    \n",
    "    #Se define el producto \n",
    "    mapaArrays = []\n",
    "    \n",
    "    times = [['15','20'],['30','30'],['45','40'],['00','00']]\n",
    "    dimTime = len(times)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i in range(dimTime):\n",
    "        productMatriz  =  []\n",
    "        for product in products:\n",
    "            j = 1 if product == 'C02' else 0   \n",
    "            ############TODO#\"###################################\n",
    "            hourTemp  = int(hour)-((i//(dimTime-1)==0))\n",
    "            filename = f'{path_base}{product}/{year}/{month}/G16_{product}_Cyl_{year}{month}{day}-{hourTemp:02d}{times[i][j]}.nc'\n",
    "            #print(filename)\n",
    "            mProduct = getMatriz(filename, origen, margen)            \n",
    "            productMatriz.append(mProduct)\n",
    "        mapaArrays.append(np.dstack(productMatriz))\n",
    "        \n",
    "    mapaArrays  = np.array(mapaArrays) \n",
    "    dato = float(tensor.numpy()[0].decode('UTF-8'))\n",
    "    if imprimir:\n",
    "        print(f\"Tiempo tomado en obtener matrices de un dato para {len(products)} productos: %.2fs\" % (time.time() - start_time))\n",
    "    \n",
    "    dato = 1 if dato>umbral else 0    \n",
    "    if len(products) == 1:\n",
    "        return mapaArrays, dato    \n",
    "    \n",
    "    return mapaArrays, dato\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3f03320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Devuelve una lista con lo indices que no se encontraron lso archivos y el producto\n",
    "#Servira para ver si se teinen todas las imagenes necesarias para el entrenamiento\n",
    "def comprobarFile(df,products,path_base):     \n",
    "    start_time = time.time()\n",
    "    no_index = []\n",
    "    no_product = []\n",
    "    no_fecha = []\n",
    "    for i in df.index:       \n",
    "        year,month,day,hour = df['fecha'][i].split('-')\n",
    "        tmpProduct = []        \n",
    "        for p in products:\n",
    "            timeDim = ['15','30','45']        \n",
    "            if p == 'C02':\n",
    "                timeDim = ['20','30','40']\n",
    "            \n",
    "            filename = f'{path_base}/{p}/{year}/{month}/G16_{p}_Cyl_{year}{month}{day}-{hour}00.nc'       \n",
    "            existe = os.path.exists(filename)\n",
    "            if not existe:\n",
    "                tmpProduct.append((p,td))\n",
    "                                      \n",
    "            for td in timeDim:\n",
    "                filename = f'{path_base}/{p}/{year}/{month}/G16_{p}_Cyl_{year}{month}{day}-{int(hour)-1 }{td}.nc'       \n",
    "                existe = os.path.exists(filename)\n",
    "                if not existe:\n",
    "                    tmpProduct.append((p,td))\n",
    "        if len(tmpProduct)>0:\n",
    "            no_index.append(i)\n",
    "            no_fecha.append(df['fecha'][i])                \n",
    "            no_product.append(tmpProduct)\n",
    "                               \n",
    "       \n",
    "    df2 = df.drop(index=no_index)\n",
    "    print(f'{len(no_index)} datos eliminados: No se encontraron los archivos de imagenes satelitales')\n",
    "    print(\"Tiempo tomado en verificar datos: %.2fs\" % (time.time() - start_time))\n",
    "    return df2 , (no_fecha,no_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "036bd060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenerDatos(filename):\n",
    "    start_time = time.time()\n",
    "    pdata = pd.read_csv(filename) \n",
    "    \n",
    "    #Seleccionamos solo las columnas necesarias : precipitacion, Estacion (Longitud), Estacion (Latitud), Fecha (año-mes-dia-hora)\n",
    "    pdataX = pdata.loc[:, ['dato','longitud', 'latitud', 'fecha']]\n",
    "\n",
    "    #Quitamos los valores NA\n",
    "    pdataX = pdataX[pdataX['dato'].notna()]\n",
    "\n",
    "    #Definimos un solo tipo (str) pora asi poder convertirlo a tensor\n",
    "    pdataX = pdataX.astype({\"dato\":str,\"longitud\":str, \"latitud\":  str, \"fecha\": str})\n",
    "                \n",
    "    #Barajeamos los datos\n",
    "    pdataX = shuffle(pdataX)\n",
    "    \n",
    "    print(f'{len(pdataX)} datos leidos')\n",
    "    print(\"Tiempo tomado en leer datos: %.2fs\" % (time.time() - start_time))\n",
    "    return pdataX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24366046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyDataset(dataset,umbral, path_base,margen,products):\n",
    "    x = []\n",
    "    y = []\n",
    "    i,j = 0.0 , []    \n",
    "    for dato in dataset:       \n",
    "        i,j =  leerImagenArea(dato,umbral, path_base,margen,products)\n",
    "        x.append(i)\n",
    "        y.append(j)\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    return x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd5e08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def train_step(x,y,model,optimizer,loss_fn,train_acc_metric):    \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cc28e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def test_step(x,y,model,val_acc_metric):      \n",
    "    val_logits = model(x, training=False)\n",
    "    val_acc_metric.update_state(y, val_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a77f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenamiento(datasetList,umbral,model,path_base,margen,products, batch_size,train_size,epocas=2,imprimir=0):  \n",
    "\n",
    "    #Dividmos el dataset (Entrenamient - Validacion)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(datasetList[:train_size])           \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(datasetList[train_size:])\n",
    "\n",
    "    #Divimos en batchs los datasets\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "    \n",
    "\n",
    "    #Definimos Variables del modelo (optmizador, funcion de loss, metricas, etc)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)        \n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "    val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "        \n",
    "    #Entrenamos el modelo\n",
    "    for epoch in range(epocas):    \n",
    "        print(\"\\nComienzo de la epoca %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for step, (datos) in enumerate(train_dataset):            \n",
    "            #Obtenmos el verdadero dataset (valor, matriz) del batch X\n",
    "            start_time_data = time.time()\n",
    "            x_train_batch, y_train_batch =  xyDataset(datos,umbral, path_base,margen,products) \n",
    "            if imprimir:\n",
    "                print(f\"Tiempo tomado para leer un batch de entrenamiento ({batch_size} datos): %.2fs\" % (time.time() - start_time_data))\n",
    "            \n",
    "            #Se obtiene el valor de perdida para el batch X\n",
    "            start_time_train = time.time()\n",
    "            loss_value = train_step(x_train_batch,y_train_batch,model,optimizer,loss_fn,train_acc_metric)\n",
    "            if imprimir:\n",
    "                print(f\"Tiempo tomado para entrenar un batch ({batch_size} datos): %.2fs\" % (time.time() - start_time_train))            \n",
    "                \n",
    "            #Log every 200 batches.\n",
    "            if step % 5 == 0:\n",
    "                print(\n",
    "                    \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                    % (step, float(loss_value))\n",
    "                )\n",
    "                print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "        \n",
    "        #Imprimimos y reiniciamos las metricas para una epoca\n",
    "        train_acc = train_acc_metric.result()\n",
    "        print(\"Training acc over epoch: %.4f\" % (float(train_acc),))        \n",
    "        train_acc_metric.reset_states()\n",
    "\n",
    "        #Usamos el dataset de validacion para la validacion\n",
    "        for (datos) in val_dataset:\n",
    "            #Verdadero dataset\n",
    "            start_time_data2 = time.time()\n",
    "            x_val_batch, y_val_batch =  xyDataset(datos,umbral, path_base,margen,products)\n",
    "            if imprimir:\n",
    "                print(f\"Tiempo tomado para leer un batch de evaluacion ({batch_size} datos): %.2fs\" % (time.time() - start_time_data2))\n",
    "                \n",
    "            start_time_evaluate = time.time()\n",
    "            #Evaluamos\n",
    "            test_step(x_val_batch, y_val_batch, model,val_acc_metric)\n",
    "            if imprimir:\n",
    "                print(f\"Tiempo tomado para evaluar un batch ({batch_size} datos): %.2fs\" % (time.time() - start_time_evaluate))\n",
    "\n",
    "        #Imprimimos y reinciamos\n",
    "        val_acc = val_acc_metric.result()\n",
    "        print(\"Validation acc: %.4f\" % (float(val_acc),))    \n",
    "        val_acc_metric.reset_states()\n",
    "        print(\"Tiempo tomado en entrenar una epoca: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d01fc2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path_base debe ser completo, se usará para comprobar si existen las imagenes satelitales descargadas\n",
    "path_base = 'C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/GOES/'\n",
    "\n",
    "#El margen servira para recortar la imagen [alto, ancho] desde el punto de origen (estacion), esta en decimales\n",
    "margen = [1,1]\n",
    "\n",
    "#POR EL MOMENTO el batch_size debe poder dividirse entre la cantidad total del dataset (no residuo) \n",
    "batch_size = 100\n",
    "\n",
    "#Representa el output del modelo (2 clases)\n",
    "#1 = Precipitacion extrema\n",
    "#0 = Precipitacion normal\n",
    "dimOutput = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3db4f3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336720 datos leidos\n",
      "Tiempo tomado en leer datos: 0.76s\n"
     ]
    }
   ],
   "source": [
    "#Leemos los datos del archivo\n",
    "#Archivo de prueba contiene datos del 2019 del mes 01 y 02\n",
    "dfOrignial = obtenerDatos('pruebasV2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fea4dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "####Se entrenara con 2 products (C08,C07)#####\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d228764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "302846 datos eliminados: No se encontraron los archivos de imagenes satelitales\n",
      "Tiempo tomado en verificar datos: 60.91s\n"
     ]
    }
   ],
   "source": [
    "#Productos de las imagenes satelitales (C08, C07 o C13, C02)\n",
    "products = ['C08','C07']\n",
    "\n",
    "#Comprobamos si existen las imagenes/produtos por cada dato,\n",
    "#caso contrario los borra de la lista\n",
    "dfVerificado, (no_i,no_p) = comprobarFile(dfOrignial,products,path_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e91a3630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "['0.0', '-72.29832', '-13.98923', '2019-01-28-15']\n"
     ]
    }
   ],
   "source": [
    "#Seleccionamos algunos para las pruebas\n",
    "df = dfVerificado[0:1000]\n",
    "datasetList = df.values.tolist()\n",
    "\n",
    "#-Visualizacion\n",
    "print(len(datasetList))\n",
    "print(datasetList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a37aa890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se creo un modelo con input (4, 110,110, 2) y output(2)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_2 (Conv3D)           (None, 2, 108, 108, 32)   1760      \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 1, 54, 54, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 93312)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                5972032   \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,973,922\n",
      "Trainable params: 5,973,922\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Creamos el modelo\n",
    "dimOutput = 2\n",
    "tempTensor =  tf.constant(datasetList[0])\n",
    "imagenT, datoT = leerImagenArea(tempTensor, 5,path_base,margen,products)\n",
    "modelo = crearModelo(imagenT.shape[0],imagenT.shape[1],imagenT.shape[2],imagenT.shape[3],dimOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "02c47c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comienzo de la epoca 0\n",
      "Training loss (for one batch) at step 0: 182.7739\n",
      "Seen so far: 100 samples\n",
      "Training loss (for one batch) at step 5: 127.1277\n",
      "Seen so far: 600 samples\n",
      "Training acc over epoch: 0.8400\n",
      "Validation acc: 0.9750\n",
      "Tiempo tomado en entrenar una epoca: 144.92s\n"
     ]
    }
   ],
   "source": [
    "#El Umbral esta en mm/h, igual que el dataset. Si supera este umbral se considera 1 (Extremo) sino 0 (no extremo)\n",
    "umbral = 2.0\n",
    "\n",
    "train_size = int(len(datasetList)*0.8)\n",
    "epocas = 1\n",
    "\n",
    "#Entrenamos con products >= 2 el dataset              \n",
    "entrenamiento(datasetList,umbral,modelo,path_base,margen,products, batch_size,train_size, epocas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d7de8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "####Se entrenara con solo 1 product (C08)#####\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "744e5b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167400 datos eliminados: No se encontraron los archivos de imagenes satelitales\n",
      "Tiempo tomado en verificar datos: 32.52s\n"
     ]
    }
   ],
   "source": [
    "#Productos de las imagenes satelitales (C08, C07 o C13, C02)\n",
    "_products = ['C08']\n",
    "\n",
    "#Comprobamos si existen las imagenes/produtos por cada dato,\n",
    "#caso contrario los borra de la lista\n",
    "_dfVerificado, (_no_i,_no_p) = comprobarFile(dfOrignial,_products,path_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4a93edfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "['0.0', '-78.82367', '-7.47987', '2019-02-05-18']\n"
     ]
    }
   ],
   "source": [
    "#Seleccionamos algunos para las pruebas\n",
    "_df = _dfVerificado[0:1000]\n",
    "_datasetList = _df.values.tolist()\n",
    "\n",
    "#-Visualizacion\n",
    "print(len(_datasetList))\n",
    "print(_datasetList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "800c5401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se creo un modelo con input (4, 110,110, 1) y output(2)\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_1 (Conv3D)           (None, 2, 108, 108, 32)   896       \n",
      "                                                                 \n",
      " max_pooling3d_1 (MaxPooling  (None, 1, 54, 54, 32)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 93312)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                5972032   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,973,058\n",
      "Trainable params: 5,973,058\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Creamos el modelo\n",
    "_tempTensor =  tf.constant(datasetList[0])\n",
    "_imagenT, _datoT = leerImagenArea(_tempTensor, 5,path_base,margen,_products)\n",
    "_modelo = crearModelo(_imagenT.shape[0],_imagenT.shape[1],_imagenT.shape[2],_imagenT.shape[3],dimOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "328de95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comienzo de la epoca 0\n",
      "Training loss (for one batch) at step 0: 16.5008\n",
      "Seen so far: 100 samples\n",
      "Training loss (for one batch) at step 5: 148.0173\n",
      "Seen so far: 600 samples\n",
      "Training acc over epoch: 0.8550\n",
      "Validation acc: 0.9900\n",
      "Tiempo tomado en entrenar una epoca: 92.22s\n"
     ]
    }
   ],
   "source": [
    "#El Umbral esta en mm/h, igual que el dataset. Si supera este umbral se considera 1 (Extremo) sino 0 (no extremo)\n",
    "umbral = 2.0\n",
    "\n",
    "train_size = int(len(datasetList)*0.8)\n",
    "epocas = 1\n",
    "\n",
    "#Entrenamos con products 1 el dataset              \n",
    "entrenamiento(_datasetList,umbral,_modelo,path_base,margen,_products, batch_size,train_size, epocas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1fe24cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "#Estadisticas de tiempo#\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73251035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo para leer matriz de un solo producto\n",
      "Tiempo tomado en obtener matrices para un dota: 0.06s\n"
     ]
    }
   ],
   "source": [
    "tempTensor = tf.constant(datasetList[0])\n",
    "print(\"Tiempo para leer matriz de un solo producto\")\n",
    "x,y = leerImagenArea(tempTensor, 5.0, path_base,margen,['C08'], 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b69207e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo para leer matriz de un solo producto\n",
      "Tiempo tomado en obtener matrices para un dota: 0.09s\n"
     ]
    }
   ],
   "source": [
    "print(\"Tiempo para leer matriz 2 productos\")\n",
    "x,y = leerImagenArea(tempTensor, 5.0, path_base,margen,['C08','C07'], 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b7241b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comienzo de la epoca 0\n",
      "Tiempo tomado para leer un batch de entrenamiento (100 datos): 5.89s\n",
      "Tiempo tomado para entrenar un batch (100 datos): 3.22s\n",
      "Training loss (for one batch) at step 0: 92.0272\n",
      "Seen so far: 100 samples\n",
      "Tiempo tomado para leer un batch de entrenamiento (100 datos): 5.74s\n",
      "Tiempo tomado para entrenar un batch (100 datos): 3.19s\n",
      "Tiempo tomado para leer un batch de entrenamiento (100 datos): 5.91s\n",
      "Tiempo tomado para entrenar un batch (100 datos): 3.21s\n",
      "Tiempo tomado para leer un batch de entrenamiento (100 datos): 5.87s\n",
      "Tiempo tomado para entrenar un batch (100 datos): 3.20s\n",
      "Tiempo tomado para leer un batch de entrenamiento (100 datos): 5.94s\n",
      "Tiempo tomado para entrenar un batch (100 datos): 3.19s\n",
      "Tiempo tomado para leer un batch de entrenamiento (100 datos): 5.88s\n",
      "Tiempo tomado para entrenar un batch (100 datos): 3.22s\n",
      "Training loss (for one batch) at step 5: 0.0000\n",
      "Seen so far: 600 samples\n",
      "Tiempo tomado para leer un batch de entrenamiento (100 datos): 5.89s\n",
      "Tiempo tomado para entrenar un batch (100 datos): 3.21s\n",
      "Tiempo tomado para leer un batch de entrenamiento (100 datos): 5.88s\n",
      "Tiempo tomado para entrenar un batch (100 datos): 3.20s\n",
      "Training acc over epoch: 0.8625\n",
      "Tiempo tomado para leer un batch de evaluacion (100 datos): 5.83s\n",
      "Tiempo tomado para evaluar un batch (100 datos): 0.35s\n",
      "Tiempo tomado para leer un batch de evaluacion (100 datos): 5.88s\n",
      "Tiempo tomado para evaluar un batch (100 datos): 0.40s\n",
      "Validation acc: 0.9900\n",
      "Tiempo tomado en entrenar una epoca: 85.10s\n"
     ]
    }
   ],
   "source": [
    "#Entrenamos con products 1 el dataset              \n",
    "entrenamiento(_datasetList,umbral,_modelo,path_base,margen,_products, batch_size,train_size, epocas,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5bbb72f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comienzo de la epoca 0\n",
      "Tiempo tomado para leer un batch de entrenamiento (100 datos): 11.72s\n",
      "Tiempo tomado para entrenar un batch (100 datos): 3.28s\n",
      "Training loss (for one batch) at step 0: 1.2648\n",
      "Seen so far: 100 samples\n",
      "Tiempo tomado para leer un batch de entrenamiento (100 datos): 11.80s\n",
      "Tiempo tomado para entrenar un batch (100 datos): 3.32s\n",
      "Tiempo tomado para leer un batch de entrenamiento (100 datos): 11.52s\n",
      "Tiempo tomado para entrenar un batch (100 datos): 3.29s\n",
      "Tiempo tomado para leer un batch de entrenamiento (100 datos): 11.77s\n",
      "Tiempo tomado para entrenar un batch (100 datos): 3.29s\n",
      "Tiempo tomado para leer un batch de entrenamiento (100 datos): 11.74s\n",
      "Tiempo tomado para entrenar un batch (100 datos): 3.26s\n",
      "Tiempo tomado para leer un batch de entrenamiento (100 datos): 11.59s\n",
      "Tiempo tomado para entrenar un batch (100 datos): 3.29s\n",
      "Training loss (for one batch) at step 5: 224.6156\n",
      "Seen so far: 600 samples\n",
      "Tiempo tomado para leer un batch de entrenamiento (100 datos): 11.65s\n",
      "Tiempo tomado para entrenar un batch (100 datos): 3.25s\n",
      "Tiempo tomado para leer un batch de entrenamiento (100 datos): 11.36s\n",
      "Tiempo tomado para entrenar un batch (100 datos): 3.29s\n",
      "Training acc over epoch: 0.8462\n",
      "Tiempo tomado para leer un batch de evaluacion (100 datos): 11.51s\n",
      "Tiempo tomado para evaluar un batch (100 datos): 0.36s\n",
      "Tiempo tomado para leer un batch de evaluacion (100 datos): 11.67s\n",
      "Tiempo tomado para evaluar un batch (100 datos): 0.35s\n",
      "Validation acc: 0.9750\n",
      "Tiempo tomado en entrenar una epoca: 143.32s\n"
     ]
    }
   ],
   "source": [
    "#Entrenamos con products 2 el dataset              \n",
    "entrenamiento(datasetList,umbral,modelo,path_base,margen,products, batch_size,train_size, epocas,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9a94e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
