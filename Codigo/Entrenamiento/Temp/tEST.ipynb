{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4390427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall visualkeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a785bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f61bb80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10d75a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16753280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b53b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "443f8291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manejo de Datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Machine learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#Librerias estandar (Extras)\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca544a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[1,1,1],[2,2,2],[3,3,3]]\n",
    "y = [[3,3,3],[4,4,4],[5,5,5]]\n",
    "#x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6eded6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 2., 2.],\n",
       "       [3., 3., 3.],\n",
       "       [4., 4., 4.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.mean( np.array([ x, y ]), axis=0 )\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d868194e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.11\n",
      "2.8.0\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "!python --version\n",
    "print(tf. __version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4728b8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda list cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eabc4c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda list cudatoolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc2ddc8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMetodos para realizar el entrenamient - evaluacion del modelo\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Metodos para realizar el entrenamient - evaluacion del modelo\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "830b35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Lee el archivo \"filename\" de datos de precipitacion y\n",
    "#regresa un df que facilite la lectura del dataset para el entrenmaiento\n",
    "def obtenerDatos(filename):\n",
    "    start_time = time.time()\n",
    "    pdata = pd.read_csv(filename)\n",
    "    \n",
    "    # Quitamos los valores NA\n",
    "    pdata = pdata[pdata['dato'].notna()]\n",
    "\n",
    "    # Definimos un solo tipo (str) pora asi poder convertirlo a tensor\n",
    "    pdata = pdata.astype({\"dato\": str, \"XO\": str, \"XA\": str, \"fecha\": str})\n",
    "\n",
    "    #Definimos la nueva columna para guardar el XO, XA y fecha\n",
    "    pdata['imagen'] = pdata.apply(obtenerDir, axis=1)\n",
    "\n",
    "    # Seleccionamos solo las columnas necesarias :\n",
    "    # precipitacion, Estacion (Longitud), Estacion (Latitud), Fecha (año-mes-dia-hora)\n",
    "    #pdataX = pdata.loc[:, ['dato','umbral','altura', 'imagen', 'fecha']]\n",
    "    pdata = pdata.astype({\"dato\": str, \"umbral\": str, \"altura\": str, \"imagen\": str, \"fecha\": str})\n",
    "\n",
    "    # Barajeamos los datos\n",
    "    pdata = shuffle(pdata)\n",
    "\n",
    "    print(f'{len(pdata)} datos leidos')\n",
    "    print(\"Tiempo tomado en leer datos: %.2fs\" % (time.time() - start_time))\n",
    "    return pdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6bfeeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Devuelve una lista con lo indices que no se encontraron lso archivos y el producto\n",
    "# Servira para ver si se teinen todas los frames de la fecha\n",
    "def comprobarFrames(dfOrignial, path_base, products, times, delete=1):\n",
    "    \n",
    "    #dfOrignial = obtenerDatos(datafile)\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    dfTotal = pd.unique(dfOrignial['fecha'])\n",
    "    no_fecha = []\n",
    "    for fecha in dfTotal:\n",
    "        year, month, day, hour = fecha.split('-')\n",
    "        existe = True\n",
    "        for p in products:\n",
    "            for t in range(len(times)):             \n",
    "                filename = f'{path_base}PNG/{fecha}/{fecha}_{t}.png'\n",
    "                try:                    \n",
    "                    file_size = os.path.getsize(filename)\n",
    "                    existe = file_size > 4100000\n",
    "                except: \n",
    "                    existe = False\n",
    "                    break\n",
    "                \n",
    "            if not existe:\n",
    "                break\n",
    "        if not existe:\n",
    "            no_fecha.append(fecha)\n",
    "            \n",
    "\n",
    "    if delete:\n",
    "        antes = len(dfOrignial)\n",
    "        df2 = dfOrignial[~dfOrignial['fecha'].isin(no_fecha)]\n",
    "        despues = len(df2)\n",
    "        print(f'{antes - despues}/{antes} datos eliminados: No se encontraron los archivos de imagenes satelitales')\n",
    "    else:\n",
    "        df2 = dfOrignial\n",
    "\n",
    "    print(\"Tiempo tomado en verificar datos: %.2fs\" % (time.time() - start_time))\n",
    "    return df2, no_fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeb4b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Del dataset guardamos los datos mas importantes en una columna para facilitar su lectura\n",
    "def obtenerDir(row):\n",
    "    fecha = row['fecha']\n",
    "\n",
    "    year, month, day, hour = fecha.split('-')\n",
    "    # filename = f'{path_base}comprimido/{year}_{month}_{day}/{hour}/'\n",
    "    return f\"{row['XO']}--{row['XA']}--{fecha}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b17ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiarDatos(listNames, path_imagenes, products, times ,delete=1):\n",
    "    df = []\n",
    "    start_time = time.time()\n",
    "    print(f'Se leera los archivos de datasets...')\n",
    "    for name in listNames:\n",
    "        try:\n",
    "            df.append(pd.read_csv(name))   \n",
    "        except:\n",
    "            print(f'No se pudo leer el archivo {name} de dataset')\n",
    "            return -1\n",
    "           \n",
    "    dsCompleto =  pd.concat(df, ignore_index=True) \n",
    "    print(\"Tiempo tomado: %.2fs\" % (time.time() - start_time))\n",
    "    print(f'+Cantidad de datos leidos {len(dsCompleto)}')\n",
    "    \n",
    "    # Quitamos los NA valores\n",
    "    print(f'Se elimnara los valores nulos')\n",
    "    dsCompleto.dropna(subset=['dato'], axis='index', inplace=True)    \n",
    "    dsCompleto = dsCompleto[dsCompleto['flag']!='ND']\n",
    "    print(\"Tiempo tomado: %.2fs\" % (time.time() - start_time))\n",
    "    print(f'+Cantidad de datos luego de elimnar nulos {len(dsCompleto)}')\n",
    "    \n",
    "    # Buscamos imagenes satelitales para lso archivos\n",
    "    print(f'Se buscara las imagenes satelitales para los datos...')\n",
    "    dfImagenes, no_fecha = comprobarFrames(dsCompleto, path_imagenes, products, times, delete)    \n",
    "    print(\"Tiempo tomado: %.2fs\" % (time.time() - start_time))\n",
    "       \n",
    "    \n",
    "    #Agregamos lso datos de las estaciones al dataset\n",
    "    print(f'Se agregara los datos de las estaciones(cordenadas, umbral)...')\n",
    "    dfImagenes['imagen'] = dfImagenes.apply(obtenerDir, axis=1)    \n",
    "    print(\"Tiempo tomado: %.2fs\" % (time.time() - start_time))\n",
    "    print(f'+Cantidad Final de datos total {len(dfImagenes)}')  \n",
    "    return shuffle(dfImagenes), no_fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1715a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo2D(p,run):    \n",
    "    print(f\"Creadno modelo con input ({p['margen'][run]},{p['margen'][run]},{p['canales'][run]})) tipo ({p['outputs']})\")\n",
    "    # Imagen\n",
    "    input_1 = tf.keras.layers.Input(shape=(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    \n",
    "    # first conv layer :\n",
    "    conv2d_1 = tf.keras.layers.Conv2D(64, kernel_size=3,activation=tf.keras.activations.relu)(input_1)\n",
    "\n",
    "    # Second conv layer :\n",
    "    conv2d_2 = tf.keras.layers.Conv2D(32, kernel_size=3,activation=tf.keras.activations.relu)(conv2d_1)\n",
    "    \n",
    "    # Flatten layer :\n",
    "    flatten = tf.keras.layers.Flatten()(conv2d_2)\n",
    "    \n",
    "    final = flatten\n",
    "    listConcat = [flatten]\n",
    "    listInputs = [input_1]\n",
    "    \n",
    "    if len(p['inputs'])>2:\n",
    "        #Agregamos los otros atrbutos        \n",
    "        for attr in p['inputs'][1:]:\n",
    "            # The other input\n",
    "            input_x = tf.keras.layers.Input(shape=(1,))\n",
    "            listConcat.append(input_x)\n",
    "            listInputs.append(input_x)\n",
    "\n",
    "            \n",
    "        # Concatenate\n",
    "        final = tf.keras.layers.Concatenate()(listConcat)\n",
    "        \n",
    "            \n",
    "        \n",
    "    # output\n",
    "    if p['outputs'] == 'dato':\n",
    "        output = tf.keras.layers.Dense(units=1, activation=tf.keras.activations.softmax)(final)\n",
    "        dimOutput = 1\n",
    "    elif p['outputs'] == 'umbral':\n",
    "        output = tf.keras.layers.Dense(units=2, activation=tf.keras.activations.relu)(final)\n",
    "        dimOutput = 2\n",
    "    else:\n",
    "        print(f\"No se pudo crear el modelo outputs no esta bien definido {p['outputs']}\")\n",
    "        return -1      \n",
    "    \n",
    "\n",
    "    full_model = tf.keras.Model(inputs=listInputs, outputs=[output])\n",
    "    \n",
    "    print('DONE')\n",
    "    \n",
    "    #print(full_model.summary())\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afbb71c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo3D(p,run):    \n",
    "    print(f\"Creando modelo con input ({p['tiempos'][run]},{p['margen'][run]},{p['margen'][run]},{p['canales'][run]})) y ({p['outputs']})...\")\n",
    "    # Imagen\n",
    "    input_1 = tf.keras.layers.Input(shape=(p['tiempos'][run],p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    \n",
    "    # first conv layer :\n",
    "    conv3d_1 = tf.keras.layers.Conv3D(64, kernel_size=3,activation=tf.keras.activations.relu)(input_1)\n",
    "\n",
    "    # Second conv layer :\n",
    "    conv3d_2 = tf.keras.layers.Conv3D(32, kernel_size=3,activation=tf.keras.activations.relu)(conv3d_1)\n",
    "    \n",
    "    # Flatten layer :\n",
    "    flatten = tf.keras.layers.Flatten()(conv3d_2)\n",
    "    \n",
    "    final = flatten\n",
    "    listConcat = [flatten]\n",
    "    listInputs = [input_1]\n",
    "    \n",
    "    if len(p['inputs'])>2:\n",
    "        #Agregamos los otros atrbutos        \n",
    "        for attr in p['inputs'][1:]:\n",
    "            # The other input            \n",
    "            input_x = tf.keras.layers.Input(shape=(1,))\n",
    "            listConcat.append(input_x)\n",
    "            listInputs.append(input_x)\n",
    "            \n",
    "        # Concatenate\n",
    "        final = tf.keras.layers.Concatenate()(listConcat)\n",
    "        \n",
    "        \n",
    "    # output\n",
    "    if p['outputs'] == 'dato':\n",
    "        output = tf.keras.layers.Dense(units=1, activation=tf.keras.activations.softmax)(final)\n",
    "        dimOutput = 1\n",
    "    elif p['outputs'] == 'umbral':\n",
    "        output = tf.keras.layers.Dense(units=2, activation=tf.keras.activations.relu)(final)\n",
    "        dimOutput = 2\n",
    "    else:\n",
    "        print(f\"No se pudo crear el modelo outputs no esta bien definido {p['outputs']}\")\n",
    "        return -1          \n",
    "\n",
    "    full_model = tf.keras.Model(inputs=listInputs, outputs=[output])\n",
    "    \n",
    "    \n",
    "    print('DONE')\n",
    "    #print(full_model.summary())\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afaa1223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo(params,run):    \n",
    "    if params['tiempos'][run] == 1:\n",
    "        #Se crea un modelo conv2D\n",
    "        return crearModelo2D(params,run)         \n",
    "    else:\n",
    "        #Se crea un modelo conv3D\n",
    "        return crearModelo3D(params,run)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ea4508f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(p, run, dataset, path_imagenes, products, times,val_split= 0.2):\n",
    "    #Dataset de etnrenamiento\n",
    "    train, test = train_test_split(dataset, test_size=val_split, shuffle=True)\n",
    "    print(f'Tamaño del dataset: Train {len(train)}  - Val {len(test)}')\n",
    "    \n",
    "    inputsList = {}\n",
    "    for inp in p['inputs']:\n",
    "        inputsList[inp] = train[inp].tolist()\n",
    "        \n",
    "       \n",
    "        \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(((inputsList),train[p['outputs']].tolist()))           \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(((inputsList),train[p['outputs']].tolist()))     \n",
    "    \n",
    "    train_dataset = train_dataset.map(lambda x ,y : read_png_file(x,y,p,run,path_imagenes,products,times))\n",
    "    val_dataset = val_dataset.map(lambda x ,y : read_png_file(x,y,p,run,path_imagenes,products,times))\n",
    "    \n",
    "    train_dataset = train_dataset.batch(p['batch']).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(p['batch']).prefetch(tf.data.AUTOTUNE)  \n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "209c22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformamos un filename tensor en una imagen\n",
    "def read_png_file(item, value, p,run, path_base, products, times):\n",
    "    # imagenData[0] = XO     # imagenData[1] = XA     # imagenData[2] = Fecha\n",
    "    imagenData = tf.strings.split(item['imagen'], sep='--')\n",
    "    size = int(p['margen'][run] / 2)\n",
    "\n",
    "    timeJoin = []\n",
    "    for j in range(p['tiempos'][run]-1,-1,-1):\n",
    "        filename = path_base + 'PNG/' + imagenData[2] + '/' + imagenData[2] + '_' + str(j) + '.png'\n",
    "        \n",
    "        image_string = tf.io.read_file(filename)\n",
    "\n",
    "        img_decoded = tf.io.decode_png(image_string, dtype=tf.uint16, channels=3)\n",
    "        #print(img_decoded.shape)\n",
    "                \n",
    "        timeJoin.insert(0,img_decoded[int(imagenData[1]) - size:int(imagenData[1]) + size,\n",
    "                                      int(imagenData[0]) - size:int(imagenData[0]) + size,\n",
    "                                      0:p['canales'][run]])\n",
    "    #return timeJoin\n",
    "        \n",
    "    if p['tiempos'][run]==1:\n",
    "        imagenData = tf.reshape(timeJoin[0],(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    else:\n",
    "        img = tf.stack(timeJoin, axis=0)\n",
    "        imagenData = tf.reshape(img,(p['tiempos'][run],p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "        \n",
    "    \n",
    "    \n",
    "    if len(p['inputs']) == 1:\n",
    "        return imagenData, int(value)\n",
    "    \n",
    "    item['imagen'] = imagenData\n",
    "    itemL = []\n",
    "    for inpL in p['inputs']:\n",
    "        itemL.append(item[inpL])\n",
    "    \n",
    "    return tuple(itemL), int(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db155bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(modelType, lr):\n",
    "    \n",
    "    if modelType == 'umbral':\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=1e-3)\n",
    "        loss_fn=keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        train_acc_metric = keras.metrics.SparseCategoricalCrossentropy()\n",
    "        val_acc_metric = keras.metrics.SparseCategoricalCrossentropy()\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=10, mode=\"max\")  \n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\"model-epoch:{epoch:02d}-loss:{loss:.3f}-val_acc:{val_acc:.3f}-weights\",\n",
    "                                                        monitor=\"val_acc\", mode=\"max\")\n",
    "        metrics = ['acc']\n",
    "        \n",
    "\n",
    "    if modelType == 'dato':\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=1e-3)\n",
    "        loss_fn=keras.losses.MeanSquaredError()\n",
    "        train_acc_metric = keras.metrics.MeanSquaredError()\n",
    "        val_acc_metric = keras.metrics.MeanSquaredError()\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_mean_squared_error\", patience=10, mode=\"max\")  \n",
    "        checkpoint = tf.keras.callbacks.ModelCheckpoint(\"model-epoch:{epoch:02d}-loss:{loss:.3f}-val_acc:{val_mean_squared_error:.3f}-weights\",\n",
    "                                                        monitor=\"val_mean_squared_error\", mode=\"max\")\n",
    "        metrics = [tf.keras.metrics.MeanSquaredError()]\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print('No se pudo crear las metricas')\n",
    "        return -1\n",
    "        \n",
    "        \n",
    "    logs = Callback()\n",
    "    callbacks = [checkpoint, early_stopping,logs]                     \n",
    "        \n",
    "    metrics = {'optimizer': optimizer, 'loss_fn':loss_fn,'train_acc_metric': train_acc_metric,\n",
    "               'val_acc_metric': val_acc_metric, 'metrics': metrics,'callbacks': callbacks}\n",
    "    \n",
    "    return metrics\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903d6e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "86a7caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(params,dataset,path_imagenes, path_base ,products, times, val_split=0.2):\n",
    "        \n",
    "    config = dict(learning_rate=params['lr'], epochs = params['epocas'],\n",
    "                     batch_size =params['batch'],architecture=\"CNN\",)\n",
    "    \n",
    "    resultados = []\n",
    "    for run in range(params['runs']):\n",
    "        history = {'loss':[],'val_loss':[],'acc':[],'val_acc':[]}        \n",
    "        #wandb.init(project='Tesis-DiegoJN', config=config, name= f\"Experimetno_{run}\")\n",
    "        \n",
    "        #Metricas y parametros de entrenaiento\n",
    "        optimizer, loss_fn, train_acc_metric, val_acc_metric, metrics = getMetrics(params['outputs'], params['lr'])\n",
    "        \n",
    "                          \n",
    "        #Modelo \n",
    "        model = crearModelo(params,run)        \n",
    "        model.compile(optimizer=optimizer,loss=loss_fn,metrics=metrics,)\n",
    "    \n",
    "        #Dataset        \n",
    "        train_dataset, val_dataset = splitDataset(params,run, dataset, path_imagenes, products, times, val_split)\n",
    "                \n",
    "        \n",
    "        print(f'Inicio de la prueba N°: {run}/{params[\"runs\"]}')        \n",
    "        print(f'- Cantidad de dataset: Train = {len(train_dataset)} - Val = {len(val_dataset)} ')\n",
    "        print(f'- Numero batch:  {params[\"batch\"]}')\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "        checkpoint_path = f'{path_base}/Modelos/{run}_{params[\"outputs\"]}_Model.epoch{params[\"epocas\"]:02d}.hdf5'        \n",
    "        \n",
    "\n",
    "        # Create a callback that saves the model's weights\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         verbose=1)\n",
    "\n",
    "        #Entrenamos\n",
    "        history = model.fit(train_dataset,batch_size=params['batch'],\n",
    "                            epochs=params['epocas'],callbacks=[logs,cp_callback],\n",
    "                            validation_data=val_dataset,\n",
    "                            validation_batch_size=params['batch'],)\n",
    "               \n",
    "               \n",
    "        \n",
    "        #wandb.finish()\n",
    "        resultados.append(history.history)\n",
    "        \"\"\"       \n",
    "        history['Product'] = products\n",
    "        history['Time'] = times\n",
    "        history['Margen'] = margen   \n",
    "        \n",
    "        #wandb.log({'epochs': epoch,\n",
    "        #           'loss': np.mean(train_loss),\n",
    "        #           'acc': float(train_acc),\n",
    "        #           'val_loss': np.mean(val_loss),\n",
    "        #           'val_acc': float(val_acc)})\n",
    "        \"\"\"\n",
    "    return resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7eac6684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nVariables generales\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Variables generales\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f05b6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables generales\n",
    "path_base = 'C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/GPUTesis'\n",
    "FAnalisis = f'{path_base}/Reportes/analisis_2020.csv'\n",
    "path_imagenes = 'F:/GOES/'\n",
    "products = ['C07','C08','C13']\n",
    "times  = ['10','20','30','40','50','00']\n",
    "listDataset = [f'{path_base}/Dataset/datasetCompleto_2020.csv',\n",
    "               f'{path_base}/Dataset/datasetCompleto_2021.csv',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a8e08a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLeemos el dataset completo\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Leemos el dataset completo\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2b09fe24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se leera los archivos de datasets...\n",
      "Tiempo tomado: 2.72s\n",
      "+Cantidad de datos leidos 2688688\n",
      "Se elimnara los valores nulos\n",
      "Tiempo tomado: 3.17s\n",
      "+Cantidad de datos luego de elimnar nulos 2491192\n",
      "Se buscara las imagenes satelitales para los datos...\n",
      "421712/2491192 datos eliminados: No se encontraron los archivos de imagenes satelitales\n",
      "Tiempo tomado en verificar datos: 5.35s\n",
      "Tiempo tomado: 8.52s\n",
      "Se agregara los datos de las estaciones(cordenadas, umbral)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shounen\\AppData\\Local\\Temp\\ipykernel_13536\\2900703187.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dfImagenes['imagen'] = dfImagenes.apply(obtenerDir, axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo tomado: 34.09s\n",
      "+Cantidad Final de datos total 2069480\n",
      "CPU times: total: 34.6 s\n",
      "Wall time: 34.7 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre</th>\n",
       "      <th>codigo</th>\n",
       "      <th>XO</th>\n",
       "      <th>XA</th>\n",
       "      <th>longitud</th>\n",
       "      <th>latitud</th>\n",
       "      <th>altura</th>\n",
       "      <th>dato</th>\n",
       "      <th>90%</th>\n",
       "      <th>99%</th>\n",
       "      <th>75%</th>\n",
       "      <th>fecha</th>\n",
       "      <th>flag</th>\n",
       "      <th>imagen</th>\n",
       "      <th>umbral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250092</th>\n",
       "      <td>CORDOVA GORE</td>\n",
       "      <td>4728F216</td>\n",
       "      <td>465</td>\n",
       "      <td>855</td>\n",
       "      <td>-75.16667</td>\n",
       "      <td>-14.03333</td>\n",
       "      <td>3181.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2020-12-19-05</td>\n",
       "      <td>C0000001</td>\n",
       "      <td>465--855--2020-12-19-05</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2423426</th>\n",
       "      <td>SANTIAGO DE TUNA</td>\n",
       "      <td>472CA750</td>\n",
       "      <td>390</td>\n",
       "      <td>742</td>\n",
       "      <td>-76.52415</td>\n",
       "      <td>-11.98311</td>\n",
       "      <td>2924.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-10-25-10</td>\n",
       "      <td>C0000001</td>\n",
       "      <td>390--742--2021-10-25-10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   nombre    codigo   XO   XA  longitud   latitud  altura  \\\n",
       "250092       CORDOVA GORE  4728F216  465  855 -75.16667 -14.03333  3181.0   \n",
       "2423426  SANTIAGO DE TUNA  472CA750  390  742 -76.52415 -11.98311  2924.0   \n",
       "\n",
       "         dato  90%  99%  75%          fecha      flag  \\\n",
       "250092    0.0  0.0  1.4  0.0  2020-12-19-05  C0000001   \n",
       "2423426   0.0  0.0  0.8  0.0  2021-10-25-10  C0000001   \n",
       "\n",
       "                          imagen  umbral  \n",
       "250092   465--855--2020-12-19-05       0  \n",
       "2423426  390--742--2021-10-25-10       0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "#Vamos a unir los dos datasets y limpiarlos\n",
    "dsCompleto, no_fecha = limpiarDatos(listDataset, path_imagenes, products, times ,1)\n",
    "\n",
    "#Agregamos el umbral al dataset\n",
    "dsCompleto['umbral'] = (dsCompleto['dato']>=dsCompleto['99%']).astype(int)\n",
    "dsCompleto.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b9ce9bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "04effa47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRealizamos los bucles\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Realizamos los bucles\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17532e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separamos para los tests\n",
    "dataset = dsCompleto\n",
    "dataset = dataset[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ecedcdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Definimos las varibles para las iteraciones\n",
    "Los parametros que van a cambiar son:\n",
    "- Canales (products)\n",
    "- Tiempos (Min de las imagenes)\n",
    "- margen\n",
    "\"\"\"\n",
    "\n",
    "params = {'inputs' : ['imagen', '99%','altura'],\n",
    "          'outputs': 'umbral',  #umbral o dato\n",
    "          'lr'     : 0.01,\n",
    "          'batch'  : 64,\n",
    "          'epocas' : 1,          \n",
    "          'canales': [3,2,3,1,2,3],\n",
    "          'tiempos': [6,1,1,6,6,6],\n",
    "          'margen' : [110,110,110,110,110,110],\n",
    "          'runs'   : 1\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "04723f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando modelo con input (6,110,110,3)) y (umbral)...\n",
      "DONE\n",
      "Tamaño del dataset: Train 800  - Val 200\n",
      "Inicio de la prueba N°: 0/1\n",
      "- Cantidad de dataset: Train = 13 - Val = 13 \n",
      "- Numero batch:  64\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.6931 - acc: 0.8537\n",
      "Epoch 1: saving model to C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/GPUTesis/Modelos\\0_umbral_Model.epoch01.hdf5\n",
      "13/13 [==============================] - 76s 6s/step - loss: 0.6931 - acc: 0.8537 - val_loss: 0.6931 - val_acc: 0.8537\n",
      "CPU times: total: 9min 58s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "resultados = trainModel(params,dataset,path_imagenes,path_base,products,times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7ec44a45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creando modelo con input (6,110,110,3)) y (umbral)...\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "modeloTest = crearModelo3D(params,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "606374ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileModelo = f'{path_base}/Modelos/0_Model.epoch64.hdf5'        \n",
    "modeloTest.load_weights(fileModelo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ba009178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=((TensorSpec(shape=(None, 6, 110, 110, 3), dtype=tf.int32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None)),)>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#imgX = np.random.randint(2000, size=(6,110,110,3)).tolist()\n",
    "imgX = tf.data.Dataset.from_tensor_slices(np.random.randint(10, size=(2,6,110,110,3)).tolist())\n",
    "umbX  = tf.data.Dataset.from_tensor_slices([0.8,0.8])\n",
    "altX = tf.data.Dataset.from_tensor_slices([1.5,0.8])\n",
    "zipped_input = tf.data.Dataset.zip(((imgX, umbX, altX), )).batch(1)\n",
    "zipped_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a94998c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for xS in zipped_input.take(1):\n",
    "    #print(xS)\n",
    "    ASd = modeloTest.predict(xS)\n",
    "    print(ASd)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
