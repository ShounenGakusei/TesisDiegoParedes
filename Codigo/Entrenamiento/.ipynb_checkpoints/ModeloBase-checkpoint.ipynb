{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73a7768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef70ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Alumno: Diego Ernesto Paredes Chilon (cod: 20155800)\n",
    "\n",
    "El objetivo principal que tiene este proyecto es desarrollar un método que\n",
    "permita optimizar parte del flujo del control de calidad de datos de\n",
    "precipitaciones realizado por SENAMHI, principalmente IDENTIFICAR LOS \n",
    "DATOS ATIPICOS DE PRECIPITACION TOMADA POR LAS ESTACIONES METEOROLOGICAS.\n",
    "\n",
    "La idea es tener un modelo de aprendizaje de maquina que pueda identificar,\n",
    "usando principalmente imagenes satelelitales, si en una determinada region/zona existe\n",
    "precipitacion extrema (atipica). \n",
    "\n",
    "Como input tendrá:\n",
    "\n",
    " - Datos de imagenes satelitales (*)\n",
    " - Altura (m.s.n.m) de las estacion meteorologica\n",
    " - Umbral definido para cada estacion meteorologica\n",
    "\n",
    "Y como output(**): \n",
    "\n",
    " - 1 = Precipitacion atipica\n",
    " - 0 = Precipitacion NO atipica\n",
    "\n",
    "\n",
    "(*) Las imagenes son previamente procesadas:  \n",
    "\n",
    "- Primero, ya que se tiene 3 canales/tipos de imagenes (3 diferentes longitudes\n",
    "electromagneticas tomadas por los satelitales), se pueden unir como una\n",
    "imagen transformandose en un archivo de 3 canales y los cuales se guardan como \n",
    "imagen de formato .png (RGB).\n",
    "\n",
    "- Segundo, debido a que los datos de precipitación son horarias y las imagenes satelitales son \n",
    "tomadas cada 10 minutos, se puede tener 6 imagenes en una hora determinada (min 10,20...00).\n",
    "Con esto se puede lograr tener un \"video\" (6 frames) para cada\n",
    "dato de precipitacion.\n",
    "\n",
    "- Finalmente, las imagenes son recortadas (110 x 100) ,  teniendo como \n",
    "centro la estacion meteorologica que tomo el dato de precipitacion a evaluar.\n",
    "\n",
    "Con esto se obtiene los datos de \"video\" con dimension (6, 110, 110 ,3)\n",
    "\n",
    "\n",
    "(**) A pesar que se tiene los datos de precipitacion en mm/h, estos se clasifican \n",
    "como atipico o no atipico (0 o 1) teniendo en cuenta un umbral. Este umbral tambien\n",
    "se lo dara como input al modelo.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------------------\n",
    "NOTA 1: Solo se ha definido el modelo base, en la fase de entrenamiento se iran variando \n",
    "algunos parametros como el numero de capas en el modelo, la cantidad de canales a usar, \n",
    "el tamaño en que se recorta la imagen, etc.\n",
    "\n",
    "NOTA 2: A pesar que tambien se piensa evaluar los resultados teniendo como output los\n",
    "datos de precipitacion en mm/h, y luego aplicar los umbrales FUERA DEL MODELO, \n",
    "se ha definido que el modelo base será el que se describio previamente, debido a \n",
    "que en este modelo ya se integra el objetivo que es identificar los valores atipicos.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11fdf1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Librerias de Machine learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "213155ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Modelo base\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_video (InputLayer)       [(None, 6, 110, 110  0           []                               \n",
      "                                , 3)]                                                             \n",
      "                                                                                                  \n",
      " conv3d_1 (Conv3D)              (None, 6, 110, 110,  2624        ['input_video[0][0]']            \n",
      "                                 32)                                                              \n",
      "                                                                                                  \n",
      " maxPool_1 (MaxPooling3D)       (None, 3, 55, 55, 3  0           ['conv3d_1[0][0]']               \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " conv3d_2 (Conv3D)              (None, 3, 55, 55, 3  27680       ['maxPool_1[0][0]']              \n",
      "                                2)                                                                \n",
      "                                                                                                  \n",
      " Flatten (Flatten)              (None, 290400)       0           ['conv3d_2[0][0]']               \n",
      "                                                                                                  \n",
      " input_altura (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_umbral (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 290402)       0           ['Flatten[0][0]',                \n",
      "                                                                  'input_altura[0][0]',           \n",
      "                                                                  'input_umbral[0][0]']           \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           18585792    ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           2080        ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " softmax (Dense)                (None, 2)            66          ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 18,618,242\n",
      "Trainable params: 18,618,242\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Input de Video (6, 110, 110, 3)\n",
    "# - Frames = 6\n",
    "# - Width  = Height = 110\n",
    "# - Canales = 3\n",
    "input_video = tf.keras.layers.Input(shape=(6,110,110,3), name= 'input_video')\n",
    "\n",
    "# Convulucional 1\n",
    "conv3d_1 = tf.keras.layers.Conv3D(32, kernel_size=3, padding='same',activation=tf.keras.activations.relu, name= 'conv3d_1')(input_video)\n",
    "maxpool_1 = tf.keras.layers.MaxPooling3D(name= 'maxPool_1')(conv3d_1)\n",
    "\n",
    "# Convulucionar 2\n",
    "conv3d_2 = tf.keras.layers.Conv3D(32, kernel_size=3, padding='same', activation=tf.keras.activations.relu, name= 'conv3d_2')(maxpool_1)\n",
    "flatten = tf.keras.layers.Flatten(name= 'Flatten')(conv3d_2)\n",
    "\n",
    "# Input de Altura (dim = 1)\n",
    "input_alt = tf.keras.layers.Input(shape=(1,), name= 'input_altura')\n",
    "\n",
    "\n",
    "# Input de Umbral (dim = 1)\n",
    "input_umb = tf.keras.layers.Input(shape=(1,), name= 'input_umbral')\n",
    "\n",
    "# Concatenamos los inputs\n",
    "concat = tf.keras.layers.Concatenate(name= 'concatenate')([flatten, input_alt, input_umb])\n",
    "\n",
    "\n",
    "#Capas densas\n",
    "dense_1 = tf.keras.layers.Dense(64, activation=tf.keras.activations.relu, name= 'dense_1')(concat)\n",
    "dense_2 = tf.keras.layers.Dense(32, activation=tf.keras.activations.relu, name= 'dense_2')(dense_1)\n",
    "\n",
    "\n",
    "#Output (2)\n",
    "# - 1 = Precipitacion Extrema\n",
    "# - 0 = Precipitacion no Extrema\n",
    "output = tf.keras.layers.Dense(units=2, activation=tf.keras.activations.softmax, name= 'softmax')(dense_2)\n",
    "\n",
    "\n",
    "#unimos del Modelo (input,output)\n",
    "full_model = tf.keras.Model(inputs=[input_video,input_alt,input_umb], outputs=[output], name='Modelo base')\n",
    "\n",
    "\n",
    "#Vemos el resultado\n",
    "print(full_model.summary())    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
