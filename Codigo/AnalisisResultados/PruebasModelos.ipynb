{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84fad3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manejo de Datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import boxcox\n",
    "\n",
    "\n",
    "#Machine learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#Librerias estandar (Extras)\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0387f861",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DIRECTORIOS DEL PROYECTO\n",
    "\"\"\"\n",
    "path_base = 'C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/GPUTesis'   # Directorio del proyecto\n",
    "path_imagenes = 'F:/GOES/'      \n",
    "\n",
    "products = ['C07','C08','C13']\n",
    "times   = ['10','20','30','40','50','00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a1a70cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo2D(p,run):    \n",
    "    # Imagen\n",
    "    input_1 = tf.keras.layers.Input(shape=(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    \n",
    "    # Convulutional layers\n",
    "    rescaling = tf.keras.layers.Rescaling(1./65536)(input_1)\n",
    "    conv2d_1 = tf.keras.layers.Conv2D(128, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(rescaling)\n",
    "    mxPool_1 = tf.keras.layers.MaxPooling2D()(conv2d_1)\n",
    "    dropout_1  = tf.keras.layers.Dropout(0.2)(mxPool_1)\n",
    "    \n",
    "    conv2d_2 = tf.keras.layers.Conv2D(64, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_1)\n",
    "    mxPool_2 = tf.keras.layers.MaxPooling2D()(conv2d_2)\n",
    "    dropout_2  = tf.keras.layers.Dropout(0.1)(mxPool_2)\n",
    "    \n",
    "    #conv2d_3 = tf.keras.layers.Conv2D(32, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_1)\n",
    "    #mxPool_3 = tf.keras.layers.MaxPooling2D()(conv2d_3)\n",
    "    #dropout_3  = tf.keras.layers.Dropout(0.2)(mxPool_3)\n",
    "    \n",
    "    #conv2d_4 = tf.keras.layers.Conv2D(64, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_3)\n",
    "    #mxPool_4 = tf.keras.layers.MaxPooling2D()(conv2d_4)\n",
    "    #dropout_4  = tf.keras.layers.Dropout(0.2)(mxPool_4)\n",
    "    \n",
    "    conv2d_5 = tf.keras.layers.Conv2D(32, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_2)\n",
    "    \n",
    "    \n",
    "    # Flatten layer :\n",
    "    flatten = tf.keras.layers.Flatten()(conv2d_5)\n",
    "    \n",
    "    final = flatten\n",
    "    listConcat = [flatten]\n",
    "    listInputs = [input_1]\n",
    "    \n",
    "    if len(p['inputs'])>1:\n",
    "        #Agregamos los otros atrbutos        \n",
    "        for attr in p['inputs'][1:]:\n",
    "            # The other input\n",
    "            input_x = tf.keras.layers.Input(shape=(1,))\n",
    "            listConcat.append(input_x)\n",
    "            listInputs.append(input_x)\n",
    "\n",
    "            \n",
    "        # Concatenate\n",
    "        final = tf.keras.layers.Concatenate()(listConcat)\n",
    "        \n",
    "    dense_1 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(final)\n",
    "    #dense_2 = tf.keras.layers.Dense(units=16, activation=tf.keras.activations.relu)(dense_1)\n",
    "    dense_3 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(dense_1)\n",
    "    \n",
    "        \n",
    "    # output\n",
    "    if p['redTipo'] == 'regresion':\n",
    "        output = tf.keras.layers.Dense(units=1)(dense_3)\n",
    "        dimOutput = 1\n",
    "    elif p['redTipo'] == 'clasificacion':\n",
    "        output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(dense_3)#units=1, activation=tf.keras.activations.relu)(dense_3)\n",
    "        dimOutput = 2\n",
    "    else:\n",
    "        print(f\"No se pudo crear el modelo outputs no esta bien definido {p['redTipo']}\")\n",
    "        return -1      \n",
    "    \n",
    "\n",
    "    full_model = tf.keras.Model(inputs=listInputs, outputs=[output])\n",
    "        \n",
    "    #print(full_model.summary())\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdf32c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo3D(p,run):        \n",
    "    # Imagen\n",
    "    input_1 = tf.keras.layers.Input(shape=(p['tiempos'][run],p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    \n",
    "    # Convulutional layers\n",
    "    rescaling = tf.keras.layers.Rescaling(1./65536)(input_1)\n",
    "    conv3d_1 = tf.keras.layers.Conv3D(128, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(rescaling)\n",
    "    #mxPool_1 = tf.keras.layers.MaxPooling3D()(conv3d_1)\n",
    "    dropout_1  = tf.keras.layers.Dropout(0.2)(conv3d_1)\n",
    "    \n",
    "    conv3d_2 = tf.keras.layers.Conv3D(64, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_1)\n",
    "    mxPool_2 = tf.keras.layers.MaxPooling3D()(conv3d_2)\n",
    "    dropout_2  = tf.keras.layers.Dropout(0.1)(mxPool_2)\n",
    "    \n",
    "    #conv2d_3 = tf.keras.layers.Conv3D(32, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_1)\n",
    "    #mxPool_3 = tf.keras.layers.MaxPooling3D()(conv2d_3)\n",
    "    #dropout_3  = tf.keras.layers.Dropout(0.2)(mxPool_3)\n",
    "    \n",
    "    #conv2d_4 = tf.keras.layers.Conv3D(64, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_3)\n",
    "    #mxPool_4 = tf.keras.layers.MaxPooling3D()(conv2d_4)\n",
    "    #dropout_4  = tf.keras.layers.Dropout(0.2)(mxPool_4)\n",
    "    \n",
    "    conv3d_5 = tf.keras.layers.Conv3D(32, kernel_size=3,padding='same',activation=tf.keras.activations.relu)(dropout_2)\n",
    "    \n",
    "    \n",
    "    # Flatten layer :\n",
    "    flatten = tf.keras.layers.Flatten()(conv3d_5)\n",
    "    \n",
    "    final = flatten\n",
    "    listConcat = [flatten]\n",
    "    listInputs = [input_1]\n",
    "    \n",
    "    if len(p['inputs'])>1:\n",
    "        #Agregamos los otros atrbutos        \n",
    "        for attr in p['inputs'][1:]:\n",
    "            # The other input\n",
    "            input_x = tf.keras.layers.Input(shape=(1,))\n",
    "            listConcat.append(input_x)\n",
    "            listInputs.append(input_x)\n",
    "\n",
    "            \n",
    "        # Concatenate\n",
    "        final = tf.keras.layers.Concatenate()(listConcat)\n",
    "        \n",
    "    dense_1 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(final)\n",
    "    #dense_2 = tf.keras.layers.Dense(units=16, activation=tf.keras.activations.relu)(dense_1)\n",
    "    dense_3 = tf.keras.layers.Dense(units=32, activation=tf.keras.activations.relu)(dense_1)\n",
    "    \n",
    "        \n",
    "    # output\n",
    "    if p['redTipo'] == 'regresion':\n",
    "        output = tf.keras.layers.Dense(units=1)(dense_3)\n",
    "        dimOutput = 1\n",
    "    elif p['redTipo'] == 'clasificacion':\n",
    "        output = tf.keras.layers.Dense(units=1,activation=tf.keras.activations.sigmoid)(dense_3)#units=1, activation=tf.keras.activations.relu)(dense_3)\n",
    "        dimOutput = 2\n",
    "    else:\n",
    "        print(f\"No se pudo crear el modelo outputs no esta bien definido {p['redTipo']}\")\n",
    "        return -1      \n",
    "    \n",
    "\n",
    "    full_model = tf.keras.Model(inputs=listInputs, outputs=[output])\n",
    "        \n",
    "    #print(full_model.summary())\n",
    "    return full_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f33adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crearModelo(params,run):    \n",
    "    if params['meanMatrizImagen']:\n",
    "        print(f\"Creando modelo 2D\")\n",
    "        print(f\"HP :(T:{params['tiempos'][run]} - M:{params['margen'][run]} - C:{params['canales'][run]}) y  tipo ({params['redTipo']})\")\n",
    "        modelo = crearModelo2D(params,run)\n",
    "    else:\n",
    "        print(f\"Creando modelo 3D\")\n",
    "        print(f\"HP :(T:{params['tiempos'][run]} - M:{params['margen'][run]} - C:{params['canales'][run]}) y  tipo ({params['redTipo']})\")\n",
    "        modelo = crearModelo3D(params,run)\n",
    "\n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbfb1251",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transformamos un filename tensor en una imagen\n",
    "def read_png_file(item, value, p,run, path_base, products, times):\n",
    "    # imagenData[0] = XO     # imagenData[1] = XA     # imagenData[2] = Fecha\n",
    "    imagenData = tf.strings.split(item['imagen'], sep='--')\n",
    "    size = int(p['margen'][run] / 2)\n",
    "\n",
    "    timeJoin = []\n",
    "    for j in range(p['tiempos'][run]-1,-1,-1):\n",
    "        filename = path_base + 'PNG/' + imagenData[2] + '/' + imagenData[2] + '_' + str(j) + '.png'\n",
    "        \n",
    "        image_string = tf.io.read_file(filename)\n",
    "\n",
    "        img_decoded = tf.io.decode_png(image_string, dtype=tf.uint16, channels=3)\n",
    "        \n",
    "        \n",
    "                \n",
    "        timeJoin.insert(0,img_decoded[int(imagenData[1]) - size:int(imagenData[1]) + size,\n",
    "                                      int(imagenData[0]) - size:int(imagenData[0]) + size,\n",
    "                                      0:p['canales'][run]])\n",
    " \n",
    "        \n",
    "    if p['tiempos'][run]==1:\n",
    "        imagenData = tf.reshape(timeJoin[0],(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "    else:\n",
    "        if p['meanMatrizImagen']:        \n",
    "            img = tf.reduce_mean( timeJoin , axis=0 )\n",
    "            imagenData = tf.reshape(img,(p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "        else:\n",
    "            img = tf.stack(timeJoin, axis=0)\n",
    "            imagenData = tf.reshape(img,(p['tiempos'][run],p['margen'][run],p['margen'][run],p['canales'][run]))\n",
    "        \n",
    "    \n",
    "    \n",
    "    if len(p['inputs']) == 1:\n",
    "        return imagenData, int(value)\n",
    "    \n",
    "    item['imagen'] = imagenData\n",
    "    itemL = []\n",
    "    for inpL in p['inputs']:\n",
    "        itemL.append(item[inpL])\n",
    "    \n",
    "    return tuple(itemL), int(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43ca2398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(p, run, ds, path_imagenes, products, times,val_split= 0.2):\n",
    "    #Dataset de etnrenamiento    \n",
    "    dataset  = ds        \n",
    "        \n",
    "    # Escojemos una fraccion del dataset si se ha indicado   \n",
    "    if p['dataset']:\n",
    "        train, test = train_test_split(dataset.sample(frac=p['dataset']), test_size=val_split, shuffle=True)\n",
    "    else:\n",
    "        train, test = train_test_split(dataset, test_size=val_split, shuffle=True)        \n",
    "        \n",
    "    print(f'TamaÃ±o del dataset: Train {len(train)}  - Val {len(test)}')\n",
    "    \n",
    "    inputsList = {}\n",
    "    for inp in p['inputs']:\n",
    "        inputsList[inp] = train[inp].tolist()              \n",
    "        \n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(((inputsList),train[p['outputs']].tolist()))           \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(((inputsList),train[p['outputs']].tolist()))     \n",
    "    \n",
    "    train_dataset = train_dataset.map(lambda x ,y : read_png_file(x,y,p,run,path_imagenes,products,times))\n",
    "    val_dataset = val_dataset.map(lambda x ,y : read_png_file(x,y,p,run,path_imagenes,products,times))\n",
    "    \n",
    "    train_dataset = train_dataset.batch(p['batch'])#.cache().prefetch(tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(p['batch'])#.prefetch(tf.data.AUTOTUNE)  \n",
    "    \n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cfc22ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluarModelo(dsPruebas,modelo, p_train):\n",
    "    inputsList = {}\n",
    "    for inp in p_train['inputs']:\n",
    "        inputsList[inp] = dsPruebas[inp].tolist()       \n",
    "\n",
    "    dsP = tf.data.Dataset.from_tensor_slices(((inputsList),dsPruebas[p_train['outputs']].tolist()))      \n",
    "    dsP = dsP.map(lambda x ,y : read_png_file(x,y,p_train,0,path_imagenes,products,times))\n",
    "    dsP = dsP.batch(p_train['batch'])#.prefetch(tf.data.AUTOTUNE)  \n",
    "    \n",
    "    hist = modelo.evaluate(dsP)\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d63a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelbyID(dsPruebas, path_base, idModel, _2D = False, inputs=['imagen', 'dato']):\n",
    "    # Buscamos el modelo en el path_base\n",
    "    os.chdir(f'{path_base}/Archivos/Modelos')\n",
    "    listFiles = list(glob.glob('**/**/**/*.hdf5'))\n",
    "    print(f'Cantidad de modelos a buscar: {len(listFiles)}')  \n",
    "        \n",
    "    \n",
    "    models = [x for x in listFiles if str(idModel) in x]\n",
    "    print(models)\n",
    "    print(idModel)\n",
    "    if len(models) > 0:\n",
    "        redTipo = models[0].split('\\\\')[0]\n",
    "\n",
    "        # Buscamos sus parametros\n",
    "        pathStats = f'{path_base}/Archivos/Reportes/Entrenamiento/{redTipo}/Reporte-{redTipo}.csv'\n",
    "        try:\n",
    "            stats = pd.read_csv(pathStats)        \n",
    "            modelStats = stats[stats['idModel']==idModel]\n",
    "            print(f'MODELO ENCONTRAOD : {modelStats}')\n",
    "        except:\n",
    "            print('No se pudo encontrar el archivo de stats o leer el id del modelo en el archivo stats')\n",
    "            print(f'Archivo leido : {pathStats}')\n",
    "            return -1\n",
    "\n",
    "\n",
    "        C,T,M = modelStats['HP'].iloc[0].split('_')        \n",
    "        p_train = {         \n",
    "                # Datos del modelo\n",
    "              'redTipo'  : redTipo.lower(), \n",
    "              'inputs'   : inputs, \n",
    "              'meanMatrizImagen' : _2D, \n",
    "              'outputs'  : 'clase', \n",
    "              'num_class': 2,\n",
    "\n",
    "               # Hiper parametros \n",
    "              'canales'  : [int(C)],\n",
    "              'tiempos'  : [int(T)],\n",
    "              'margen'   : [int(M)],\n",
    "              'runs'     : 1,\n",
    "            \n",
    "              # Entrenamiento\n",
    "              'batch'    : 32,\n",
    "              'lr'       : 0.001,\n",
    "         }\n",
    "        \n",
    "        modelo = crearModelo(p_train,0)\n",
    "        modelo.load_weights(models[0])\n",
    "        metricas = getMetrics(redTipo.lower(),p_train['lr'], 7)\n",
    "        modelo.compile(optimizer=metricas['optimizer'],loss=metricas['loss_fn'],metrics=metricas['metrics'],)\n",
    "        \n",
    "        hist = evaluarModelo(dsPruebas,modelo, p_train)\n",
    "        return modelo, p_train, hist\n",
    "        \n",
    "    else:\n",
    "        print(f'No se encontro el modelo con id {idModel} en la PC')\n",
    "        return -1\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "925b78e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMetrics(modelType, lr, paciencia):\n",
    "    \n",
    "    if modelType == 'clasificacion':    \n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr) \n",
    "        \n",
    "        #BinaryCrossentropy() #CategoricalCrossentropy()      \n",
    "        loss_fn= keras.losses.BinaryCrossentropy()\n",
    "        train_acc_metric = keras.metrics.BinaryCrossentropy()\n",
    "        val_acc_metric = keras.metrics.BinaryCrossentropy()\n",
    "        if paciencia:\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=paciencia, mode=\"max\")  \n",
    " \n",
    "        \n",
    "        metrics = ['acc', keras.metrics.TruePositives(),\n",
    "                         keras.metrics.TrueNegatives(),\n",
    "                         keras.metrics.FalsePositives(),\n",
    "                         keras.metrics.FalseNegatives()]\n",
    "        \n",
    "\n",
    "    elif modelType == 'regresion':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=lr)\n",
    "        loss_fn=keras.losses.MeanSquaredError()\n",
    "        train_acc_metric = keras.metrics.MeanSquaredError()\n",
    "        val_acc_metric = keras.metrics.MeanSquaredError()\n",
    "        if paciencia:\n",
    "            early_stopping = keras.callbacks.EarlyStopping(monitor=\"val_mse\", patience=paciencia, mode=\"max\")                                            \n",
    "        metrics = ['mse']\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print('No se pudo crear las metricas')\n",
    "        return -1    \n",
    "         \n",
    "        \n",
    "    logs = Callback()\n",
    "    callbacks = [logs]                     \n",
    "    if paciencia:\n",
    "        callbacks.append(early_stopping)\n",
    "        \n",
    "    metrics = {'optimizer': optimizer, 'loss_fn':loss_fn,'train_acc_metric': train_acc_metric,\n",
    "               'val_acc_metric': val_acc_metric, 'metrics': metrics,'callbacks': callbacks}\n",
    "    \n",
    "    return metrics\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c6765a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruebas para modelos convulucional 3D\n",
      "Cantidad de pruebas : 367\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>nombre</th>\n",
       "      <th>codigo</th>\n",
       "      <th>XO</th>\n",
       "      <th>XA</th>\n",
       "      <th>longitud</th>\n",
       "      <th>latitud</th>\n",
       "      <th>altura</th>\n",
       "      <th>dato</th>\n",
       "      <th>90%</th>\n",
       "      <th>99%</th>\n",
       "      <th>75%</th>\n",
       "      <th>fecha</th>\n",
       "      <th>flag</th>\n",
       "      <th>flagV2</th>\n",
       "      <th>imagen</th>\n",
       "      <th>clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LA FORTUNA</td>\n",
       "      <td>107131</td>\n",
       "      <td>286</td>\n",
       "      <td>502</td>\n",
       "      <td>-78.40242</td>\n",
       "      <td>-7.67042</td>\n",
       "      <td>3343.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>5.357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-12-27-11</td>\n",
       "      <td>C0000002</td>\n",
       "      <td>C01</td>\n",
       "      <td>286--502--2021-12-27-11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>TUNEL CHOTANO</td>\n",
       "      <td>47E1F02E</td>\n",
       "      <td>265</td>\n",
       "      <td>440</td>\n",
       "      <td>-78.78432</td>\n",
       "      <td>-6.54049</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-12-11-23</td>\n",
       "      <td>M0000002</td>\n",
       "      <td>C01</td>\n",
       "      <td>265--440--2021-12-11-23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         nombre    codigo   XO   XA  longitud  latitud  altura  \\\n",
       "0           0     LA FORTUNA    107131  286  502 -78.40242 -7.67042  3343.0   \n",
       "1           1  TUNEL CHOTANO  47E1F02E  265  440 -78.78432 -6.54049  2027.0   \n",
       "\n",
       "   dato  90%    99%  75%          fecha      flag flagV2  \\\n",
       "0   0.0  0.4  5.357  0.0  2021-12-27-11  C0000002    C01   \n",
       "1   0.0  0.1  3.939  0.0  2021-12-11-23  M0000002    C01   \n",
       "\n",
       "                    imagen  clase  \n",
       "0  286--502--2021-12-27-11      1  \n",
       "1  265--440--2021-12-11-23      0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# leemos el DS\n",
    "print('Pruebas para modelos convulucional 3D')\n",
    "pruebasFile = f'{path_base}/Archivos/Dataset/Pruebas/CLASEV2-C3_PruebasDS.csv'\n",
    "#pruebasFile = f'{path_base}/Archivos/Dataset/SplittedV2/CLASE_TrainDS_2.csv'\n",
    "#pruebasFile = f'{path_base}/Archivos/Dataset/CLASE_TrainDS.csv'\n",
    "dsPruebas = pd.read_csv(pruebasFile)\n",
    "print(f'Cantidad de pruebas : {len(dsPruebas)}')  \n",
    "dsPruebas.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48c538e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shounen\\AppData\\Local\\Temp\\ipykernel_108\\751691209.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  dsPruebasS = dsPruebasS[dsPruebas['flag']=='C0000002']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['C0000002'], dtype=object)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsPruebasS = dsPruebas[dsPruebas['flagV2']=='D02']\n",
    "dsPruebasS = dsPruebasS[dsPruebas['flag']=='C0000002']\n",
    "len(dsPruebasS)\n",
    "dsPruebasS['flag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a7a5779",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dsPruebas = dsPruebas[dsPruebas['flag']=='C0000002']\n",
    "dsPruebas = dsPruebas[dsPruebas['dato']!=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48f1e040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de modelos a buscar: 125\n",
      "['Clasificacion\\\\TesisDiego3D_V3\\\\CLASE_TrainDS_15\\\\Model_clasificacion_clase_20220429_180712.hdf5']\n",
      "20220429_180712\n",
      "MODELO ENCONTRAOD : Empty DataFrame\n",
      "Columns: [Unnamed: 0, dsDir, idModel, HP, DS_len, dsName, epoca, val_TN, val_TP, val_FP, val_FN, val_acc, TNR, TPR, PPV, NPV, F1]\n",
      "Index: []\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m modelo, p_train, hist \u001b[38;5;241m=\u001b[39m \u001b[43mgetModelbyID\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsPruebas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m20220429_180712\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_2D\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mgetModelbyID\u001b[1;34m(dsPruebas, path_base, idModel, _2D, inputs)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArchivo leido : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpathStats\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 26\u001b[0m C,T,M \u001b[38;5;241m=\u001b[39m \u001b[43mmodelStats\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)        \n\u001b[0;32m     27\u001b[0m p_train \u001b[38;5;241m=\u001b[39m {         \n\u001b[0;32m     28\u001b[0m         \u001b[38;5;66;03m# Datos del modelo\u001b[39;00m\n\u001b[0;32m     29\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mredTipo\u001b[39m\u001b[38;5;124m'\u001b[39m  : redTipo\u001b[38;5;241m.\u001b[39mlower(), \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m       : \u001b[38;5;241m0.001\u001b[39m,\n\u001b[0;32m     44\u001b[0m  }\n\u001b[0;32m     46\u001b[0m modelo \u001b[38;5;241m=\u001b[39m crearModelo(p_train,\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\pandas\\core\\indexing.py:967\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    964\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    966\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m--> 967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\pandas\\core\\indexing.py:1520\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[1;32m-> 1520\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\WB-PY39\\lib\\site-packages\\pandas\\core\\indexing.py:1452\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1450\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[0;32m   1451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[1;32m-> 1452\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "modelo, p_train, hist = getModelbyID(dsPruebas, path_base, '20220429_180712' , _2D = True)\n",
    "#20220427_151059 \n",
    "#20220429_180712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab98223",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa1d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsPruebas['dato'].hist()\n",
    " \n",
    "print(f'Cantidad total: {dsPruebas[\"dato\"].count()}')\n",
    "print(f'Cantidad ceros: {dsPruebas[dsPruebas[\"dato\"]==0][\"dato\"].count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f21a6c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'F:/GOES/PNG/2020-01-01-04/2020-01-01-04_1.png'\n",
    "image_string = tf.io.read_file(filename)\n",
    "img_decoded = tf.io.decode_png(image_string, dtype=tf.uint16, channels=3)       \n",
    "img_decoded = tf.image.flip_left_right(img_decoded)\n",
    "\n",
    "#plt.imshow(img_decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa235a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
