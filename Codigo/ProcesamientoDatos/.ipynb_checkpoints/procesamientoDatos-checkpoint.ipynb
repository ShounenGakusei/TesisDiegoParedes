{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5003e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "939612b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f500a04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2380c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "299bb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si se desea graficar\n",
    "#!pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4529a7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df4b9330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset, num2date\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70311502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DEFINIMOS EL PATH BASE DEL PROYECTO\n",
    "\"\"\"\n",
    "path_base = 'C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/GPUTesis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb92709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para un conjunto de estaciones (dataframe), agrega su posicion XO(longitud), XA(latitud)\n",
    "\n",
    "#Como input recibe un string que es el directoeio de una imagen satelital (.nc),\n",
    "#devuelve la matriz que representa esta imagen numpy.array(1200,950)\n",
    "def getMapFile(imagenFile):    \n",
    "    try:\n",
    "        ds = Dataset(imagenFile)      \n",
    "    except:\n",
    "        print(\"No se pudo leer los archivos de imagen\")\n",
    "        print(imagenFile)\n",
    "        return -1,-1\n",
    "\n",
    "    # obtiene las coordenadas de los pixeles\n",
    "    lons = ds.variables['longitude'][:].data\n",
    "    lats = ds.variables['latitude'][:].data            \n",
    "        \n",
    "    return lons, lats  \n",
    "\n",
    "#Busca el valor X en el array, devuelve su posicion\n",
    "def getPosMap(x,array):    \n",
    "    pos = -1\n",
    "    for i in range(len(array)):\n",
    "        if abs(array[i]-x)<=0.01:\n",
    "            pos = i\n",
    "            \n",
    "    return  pos  \n",
    "\n",
    "#en el la imagen satelital\n",
    "def changeOrigenStation(estaciones,imagenFile):\n",
    "    try:\n",
    "        station = pd.read_csv(estaciones)   \n",
    "    except:\n",
    "        print(\"No se pudo leer el  archivos de estaciones\")\n",
    "        return False\n",
    "    \n",
    "    lo,la = getMapFile(imagenFile)\n",
    "    \n",
    "    station['XO'] = station.apply(lambda x: getPosMap(float(x['LON']), lo),axis=1)\n",
    "    station['XA'] = station.apply(lambda x: getPosMap(float(x['LAT']), la),axis=1)    \n",
    "    \n",
    "    \n",
    "    return station    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da420d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtiene los datos de precipitacion de un archivo csv, los procesa y los guarda en OTRO archivo CSV\n",
    "#regresa la cantidad de estaciones sin datos de precipitaciones\n",
    "def procesarDatos(path_base,estaciones,valores,imagenFile,noFLAG,nameOut):  \n",
    "    start_time = time.time()\n",
    "    #Obtenemos la informacion de los archivos\n",
    "    #\"Valores\"  Contiene los valores de los datos de precipitacion de manera horaria por estacion (codigo)\n",
    "    #\"estaciones\" Contiene los daots de cada estacion (cordenadas,codigo,etc)  \n",
    "    try:        \n",
    "        values = pd.read_csv(valores,encoding='latin-1')    \n",
    "    except:\n",
    "        print(\"No se pudo leer el archivos de valores\")\n",
    "        return False\n",
    "    station = changeOrigenStation(estaciones,imagenFile)\n",
    "    \n",
    "    \n",
    "    #Lista de [nombre,codigo,xo,xa,longitud,latitud,altura,dato,año,mes,dia,hora,flag]\n",
    "    resultado = []\n",
    "    \n",
    "    #Numero filas\n",
    "    n = len(values.index) \n",
    "    \n",
    "    #Auxiliares\n",
    "    total = n\n",
    "    completados = 1 \n",
    "    \n",
    "    noStation = {}\n",
    "    \n",
    "    for i in range(n):\n",
    "        #Codigo de estacion\n",
    "        cod = values['CODIGO'][i]        \n",
    "        st = station[station['Codigo'] == f'X{cod}']\n",
    "        if st.empty:            \n",
    "            st = station[station['Nombre'] == values['NOMBRE'][i]]\n",
    "        if st.empty:\n",
    "            noStation[cod] = values['NOMBRE'][i]\n",
    "            \n",
    "        if not st.empty:       \n",
    "            flag = values['FLAG'][i]\n",
    "            \n",
    "            if flag not in noFLAG:\n",
    "                #Datos de estacion\n",
    "                nombre = st['Nombre'].iloc[0]\n",
    "                xo = st['XO'].iloc[0]\n",
    "                xa =  st['XA'].iloc[0]\n",
    "                lat = st['LAT'].iloc[0]\n",
    "                lon = st['LON'].iloc[0]\n",
    "                alt = st['ALT'].iloc[0]\n",
    "                \n",
    "                _90 = st['90%'].iloc[0]\n",
    "                _99 = st['99%'].iloc[0]\n",
    "                _75 = st['75%'].iloc[0]\n",
    "                \n",
    "                flagV2 =  values['FLAGV2'][i]\n",
    "                #Datos de precipitacion\n",
    "                dato = values['PRECIPITACION'][i]        \n",
    "                day, month , year = values['FECHA'][i].split('/')   \n",
    "                hour = values['HORA'][i].split(':')[0]                    \n",
    "\n",
    "                resultado.append([nombre,cod,xo,xa,lon,lat,alt,dato,_90,_99,_75,f'{year}-{month}-{day}-{hour}',flag,flagV2])\n",
    "                completados = completados + 1\n",
    "        \n",
    "        if not completados % 100000:\n",
    "            progreso = completados/total*100\n",
    "            print(f'Estaciones con Error: {len(noStation)}')\n",
    "            f = '{0:.3g}'.format(progreso)\n",
    "            print(f\"Procesando - {f}%\")\n",
    "    \n",
    "    print(f\"Tiempo tomado en procesar {completados}/{n} datos: %.5fs\" % (time.time() - start_time))\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #Guardamos en un csv los datos \n",
    "    print(\"Guardando los datos al archivo dataset....\")\n",
    "    df = pd.DataFrame(resultado, columns = ['nombre','codigo','XO','XA','longitud','latitud','altura','dato','90%','99%','75%','fecha','flag','flagV2'])\n",
    "    df.to_csv(f'{path_base}/Archivos/Dataset/datasetCompleto_{nameOut}.csv', index=False)\n",
    "    print(f\"Tiempo tomado en guardar {completados}datos: %.5fs\" % (time.time() - start_time))\n",
    "    return noStation\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fae0cc66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nProcesamos los datos de precipitacion para cada año\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Procesamos los datos de precipitacion para cada año\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1bb82afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Festaciones = f'{path_base}/Archivos/Inicial/estaciones.csv'\n",
    "\n",
    "Fdatos2021 = f'{path_base}/Archivos/Inicial/REPORTE_SGD_2021.csv'\n",
    "Fdatos2020 = f'{path_base}/Archivos/Inicial/REPORTE_SGD_2020.csv'\n",
    "\n",
    "FflagV2 = f'{path_base}/Archivos/Dataset/dsFLAGV2.csv'\n",
    "\n",
    "imagenTest = 'F:/GOES/C08/2021/01/G16_C08_Cyl_20210101-0020.nc'\n",
    "Fanalisis = f'{path_base}/Archivos/Reportes/analisis_2021.csv'\n",
    "\n",
    "#Flag que se borran del dataset\n",
    "noFLAG = ['M0000001','M0000001, M0110302','M0110302','ND']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f4398dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAño 2021 \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Año 2021 \n",
    "\"\"\"\n",
    "#Se procesa los datos de precipitacion para el año 2021\n",
    "#noStation2021 = procesarDatos(path_base,Fanalisis,Fdatos2021,imagenTest,noFLAG,'2021-T2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deec6de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobamos el formato y los del archivo creado (2021)\n",
    "#datos2021 = pd.read_csv(f'{path_base}/Archivos/Dataset/datasetCompleto_2021-T2.csv')\n",
    "#datos2021.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52f103a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAño 2021 \\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Año 2021 \n",
    "\"\"\"\n",
    "#Se procesa los datos de precipitacion para el año 2020\n",
    "#noStation2020 = procesarDatos(path_base,Fanalisis,Fdatos2020,imagenTest,noFLAG,'2020-T2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b6acf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comprobamos el formato y los del archivo creado (2021)\n",
    "#datos2020 = pd.read_csv(f'{path_base}/Archivos/Dataset/datasetCompleto_2020-T2.csv')\n",
    "#datos2020.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3b99896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFLAGV2 \\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "FLAGV2 \n",
    "\"\"\"\n",
    "#noStation2020 = procesarDatos(path_base,Fanalisis,FflagV2,imagenTest,noFLAG,'FLAGV2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5dc3af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datosFLAGV2 = pd.read_csv(f'{path_base}/Archivos/Dataset/datasetCompleto_FLAGV2.csv')\n",
    "#datosFLAGV2.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6656f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datosFLAGV2['flagV2'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8d8a3a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nLimpiamos el dataset\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Limpiamos el dataset\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6425d7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Lee el archivo \"filename\" de datos de precipitacion y\n",
    "#regresa un df que facilite la lectura del dataset para el entrenmaiento\n",
    "def obtenerDatos(filename):\n",
    "    start_time = time.time()\n",
    "    pdata = pd.read_csv(filename)\n",
    "    \n",
    "    # Quitamos los valores NA\n",
    "    pdata = pdata[pdata['dato'].notna()]\n",
    "\n",
    "    # Definimos un solo tipo (str) pora asi poder convertirlo a tensor\n",
    "    pdata = pdata.astype({\"dato\": str, \"XO\": str, \"XA\": str, \"fecha\": str})\n",
    "\n",
    "    #Definimos la nueva columna para guardar el XO, XA y fecha\n",
    "    pdata['imagen'] = pdata.apply(obtenerDir, axis=1)\n",
    "\n",
    "    # Seleccionamos solo las columnas necesarias :\n",
    "    # precipitacion, Estacion (Longitud), Estacion (Latitud), Fecha (año-mes-dia-hora)\n",
    "    #pdataX = pdata.loc[:, ['dato','umbral','altura', 'imagen', 'fecha']]\n",
    "    pdata = pdata.astype({\"dato\": str, \"umbral\": str, \"altura\": str, \"imagen\": str, \"fecha\": str})\n",
    "\n",
    "    # Barajeamos los datos\n",
    "    pdata = shuffle(pdata)\n",
    "\n",
    "    print(f'{len(pdata)} datos leidos')\n",
    "    print(\"Tiempo tomado en leer datos: %.2fs\" % (time.time() - start_time))\n",
    "    return pdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d9befc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Devuelve una lista con lo indices que no se encontraron lso archivos y el producto\n",
    "# Servira para ver si se teinen todas los frames de la fecha\n",
    "def comprobarFrames(dfOrignial, path_base, products, times, delete=1):\n",
    "    \n",
    "    #dfOrignial = obtenerDatos(datafile)\n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    dfTotal = pd.unique(dfOrignial['fecha'])\n",
    "    no_fecha = []\n",
    "    for fecha in dfTotal:\n",
    "        year, month, day, hour = fecha.split('-')\n",
    "        existe = True\n",
    "        for p in products:\n",
    "            for t in range(len(times)):             \n",
    "                filename = f'{path_base}PNG/{fecha}/{fecha}_{t}.png'\n",
    "                try:                    \n",
    "                    file_size = os.path.getsize(filename)\n",
    "                    existe = file_size > 4100000\n",
    "                except: \n",
    "                    existe = False\n",
    "                    break\n",
    "                \n",
    "            if not existe:\n",
    "                break\n",
    "        if not existe:\n",
    "            no_fecha.append(fecha)\n",
    "            \n",
    "\n",
    "    if delete:\n",
    "        antes = len(dfOrignial)\n",
    "        df2 = dfOrignial[~dfOrignial['fecha'].isin(no_fecha)]\n",
    "        despues = len(df2)\n",
    "        print(f'{antes - despues}/{antes} datos eliminados: No se encontraron los archivos de imagenes satelitales')\n",
    "    else:\n",
    "        df2 = dfOrignial\n",
    "\n",
    "    print(\"Tiempo tomado en verificar datos: %.2fs\" % (time.time() - start_time))\n",
    "    return df2, no_fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebaeaec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Del dataset guardamos los datos mas importantes en una columna para facilitar su lectura\n",
    "def obtenerDir(row):\n",
    "    fecha = row['fecha']\n",
    "\n",
    "    year, month, day, hour = fecha.split('-')\n",
    "    # filename = f'{path_base}comprimido/{year}_{month}_{day}/{hour}/'\n",
    "    return f\"{row['XO']}--{row['XA']}--{fecha}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25cfdacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBalancedDataset(df,porc):\n",
    "    listP = []\n",
    "    listT = []    \n",
    "    \n",
    "    df0 = df[df['dato']==0]\n",
    "    df01 = df[(df['dato']>0) & (df['dato']<=1)]\n",
    "    df1 = df[df['dato']>1]\n",
    "    \n",
    "    if not df0.empty:\n",
    "        dsT0, dsP0 = train_test_split(df0, test_size=porc, shuffle=True)  \n",
    "        listT.append(dsT0)\n",
    "        listP.append(dsP0)        \n",
    "        \n",
    "    if not df01.empty:\n",
    "        dsT1,dsP1 = train_test_split(df01, test_size=porc, shuffle=True)\n",
    "        listT.append(dsT1)\n",
    "        listP.append(dsP1)\n",
    "        \n",
    "    if not df1.empty:\n",
    "        dsT_, dsP_ =  train_test_split(df1, test_size=porc, shuffle=True)\n",
    "        listT.append(dsT_)\n",
    "        listP.append(dsP_)\n",
    "    \n",
    "   \n",
    "    dfTrain = pd.concat(listT, ignore_index=True) \n",
    "    dfPrueba = pd.concat(listP, ignore_index=True)\n",
    "    return dfTrain, dfPrueba    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2381206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenerDSPruebas(df, porc, tipo='clasificacion'):\n",
    "    \"\"\"\n",
    "    Separamos el dataset en 3 partes, y se ecoje un porcentaje de los tres\n",
    "    - Datos : 0\n",
    "    - Dato  : < 0,1 ]\n",
    "    - Dato  : <1 , --]\n",
    "    \"\"\"\n",
    "    \n",
    "    if tipo=='clasificacion':\n",
    "        dfC02 = df[df['flag']=='C0000002'] \n",
    "        dfM02 = df[df['flag']=='M0000002']\n",
    "    \n",
    "        C02T,C02P = getBalancedDataset(dfC02,porc)\n",
    "        M02T,M02P = getBalancedDataset(dfM02,porc)   \n",
    "    \n",
    "   \n",
    "        dfTrain = pd.concat([C02T,M02T], ignore_index=True) \n",
    "        dfPrueba = pd.concat([C02P,M02P], ignore_index=True) \n",
    "        \n",
    "        return dfTrain, dfPrueba\n",
    "        \n",
    "    else:        \n",
    "        return getBalancedDataset(df,porc)\n",
    "    \n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c496440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpiarDatos(listNames, path_imagenes, p):\n",
    "    \n",
    "    df = []\n",
    "    start_time = time.time()\n",
    "    print(f'Se leera los archivos de datasets...')\n",
    "    for name in listNames:\n",
    "        try:\n",
    "            df.append(pd.read_csv(name))   \n",
    "        except:\n",
    "            print(f'No se pudo leer el archivo {name} de dataset')\n",
    "            return -1\n",
    "        \n",
    "    if len(df)>1:\n",
    "        dsCompleto =  pd.concat(df, ignore_index=True) \n",
    "    else:\n",
    "        dsCompleto =  df[0]\n",
    "        \n",
    "    print(\"Tiempo tomado: %.2fs\" % (time.time() - start_time))\n",
    "    print(f'+Cantidad de datos leidos {len(dsCompleto)}')\n",
    "    \n",
    "    # Quitamos los NA valores, negativos y mayores a 400\n",
    "    print(f'\\nSe elimnara los valores nulos y dudosos')\n",
    "    dsCompleto.dropna(subset=['dato'], axis='index', inplace=True)    \n",
    "    dsCompleto = dsCompleto[dsCompleto['flag']!='ND']\n",
    "    dsCompleto = dsCompleto[dsCompleto['dato']>=0]\n",
    "    dsCompleto = dsCompleto[dsCompleto['dato']<400]\n",
    "    print(\"Tiempo tomado: %.2fs\" % (time.time() - start_time))\n",
    "    print(f'+Cantidad de datos luego de elimnar nulos {len(dsCompleto)}')\n",
    "              \n",
    "    \n",
    "    # Seleccionamos FLAGS\n",
    "    if p['flags']:\n",
    "        dsCompleto = dsCompleto[dsCompleto['flag'].isin(p['flags'])]    \n",
    "    if p['flagV2']:\n",
    "        dsCompleto = dsCompleto[dsCompleto['flagV2'].isin(p['flagV2'])]    \n",
    "    \n",
    "    \n",
    "    # Buscamos imagenes satelitales para lso archivos\n",
    "    print(f'\\nSe buscara las imagenes satelitales para los datos...')\n",
    "    dfImagenes, no_fecha = comprobarFrames(dsCompleto, path_imagenes, p['products'], p['times'], p['delete'])    \n",
    "    print(\"Tiempo tomado: %.2fs\" % (time.time() - start_time))\n",
    "               \n",
    "        \n",
    "    # Limitamos la cantidad de ceros\n",
    "    if p['ceros']:\n",
    "        print(f'\\nSe limitara el numero de ceros al {p[\"ceros\"]*100:.2f} porciento...')\n",
    "        dsNoNulo = dfImagenes[dfImagenes['dato']!=0]\n",
    "        dsNulo = dfImagenes[dfImagenes['dato']==0]\n",
    "        dsNulo = dsNulo.sample(frac=p['ceros'])#n=int(p['ceros']*len(dsNoNulo)))        \n",
    "        dfImagenes = pd.concat([dsNoNulo,dsNulo], ignore_index=True) \n",
    "        print(f'+Cantidad de datos luego de limtar ceros {len(dfImagenes)}')\n",
    "            \n",
    "    #Agregamos lso datos de las estaciones al dataset\n",
    "    print(f'\\nSe agregara los datos de las estaciones(cordenadas, umbral)...')\n",
    "    dfImagenes['imagen'] = dfImagenes.apply(obtenerDir, axis=1)        \n",
    "    print(f'-------> CANTIDAD FINAL DE DATOS :  {len(dfImagenes)} <--------------')  \n",
    "    print(\"Tiempo total: %.2fs\" % (time.time() - start_time))\n",
    "    return shuffle(dfImagenes), no_fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "caf93a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_imagenes = 'F:/GOES/' \n",
    "\n",
    "# Directorio de las imagenes procesadas\n",
    "listDataset = [f'{path_base}/Archivos/Dataset/datasetCompleto_FLAGV2.csv']#,\n",
    "              # f'{path_base}/Archivos/Dataset/datasetCompleto_2021-T2.csv',\n",
    "              # f'{path_base}/Archivos/Dataset/datasetCompleto_2020-T2.csv']          # Directorio(s) de los datos de precipitacion\n",
    "\n",
    "#Variables generales\n",
    "products = ['C07','C08','C13']\n",
    "times   = ['10','20','30','40','50','00']\n",
    "        \n",
    "        \n",
    "p_dataset = {\n",
    "    # parametros del proyecto\n",
    "    'products' : products,\n",
    "    'times'  : times,\n",
    "    'path_base' : path_base,\n",
    "\n",
    "    # parametros del dataset\n",
    "    'ceros' : 0.10,   \n",
    "    'flags' : ['C0000002','DIM00001','M0000002'],\n",
    "    'flagV2': ['D01','D02','C01'],    \n",
    "    'delete' : True,\n",
    "    \n",
    "    # Name\n",
    "    'redTipo': 'Clasificacion',\n",
    "    'nameDS' : 'Clase_DA_'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "964981df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se leera los archivos de datasets...\n",
      "Tiempo tomado: 2.57s\n",
      "+Cantidad de datos leidos 2494399\n",
      "\n",
      "Se elimnara los valores nulos y dudosos\n",
      "Tiempo tomado: 3.36s\n",
      "+Cantidad de datos luego de elimnar nulos 2494117\n",
      "\n",
      "Se buscara las imagenes satelitales para los datos...\n",
      "1992/45264 datos eliminados: No se encontraron los archivos de imagenes satelitales\n",
      "Tiempo tomado en verificar datos: 3.06s\n",
      "Tiempo tomado: 6.53s\n",
      "\n",
      "Se limitara el numero de ceros al 10.00 porciento...\n",
      "+Cantidad de datos luego de limtar ceros 34668\n",
      "\n",
      "Se agregara los datos de las estaciones(cordenadas, umbral)...\n",
      "-------> CANTIDAD FINAL DE DATOS :  34668 <--------------\n",
      "Tiempo total: 7.02s\n",
      "CPU times: total: 7.03 s\n",
      "Wall time: 7.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Vamos a unir los dos datasets y limpiarlos\n",
    "dsCompleto, no_fecha = limpiarDatos(listDataset, path_imagenes, p_dataset)\n",
    "dsCompleto['clase']  =(dsCompleto['flag']=='C0000002').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "990af3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN   : 32597\n",
      "VALID   : 1720\n",
      "PRUEBAS : 349\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nombre</th>\n",
       "      <th>codigo</th>\n",
       "      <th>XO</th>\n",
       "      <th>XA</th>\n",
       "      <th>longitud</th>\n",
       "      <th>latitud</th>\n",
       "      <th>altura</th>\n",
       "      <th>dato</th>\n",
       "      <th>90%</th>\n",
       "      <th>99%</th>\n",
       "      <th>75%</th>\n",
       "      <th>fecha</th>\n",
       "      <th>flag</th>\n",
       "      <th>flagV2</th>\n",
       "      <th>imagen</th>\n",
       "      <th>clase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COSPAN</td>\n",
       "      <td>472D4658</td>\n",
       "      <td>278</td>\n",
       "      <td>489</td>\n",
       "      <td>-78.54106</td>\n",
       "      <td>-7.42856</td>\n",
       "      <td>2423.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-12-03-13</td>\n",
       "      <td>C0000002</td>\n",
       "      <td>C01</td>\n",
       "      <td>278--489--2021-12-03-13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CASCABAMBA</td>\n",
       "      <td>472C92CA</td>\n",
       "      <td>268</td>\n",
       "      <td>486</td>\n",
       "      <td>-78.72682</td>\n",
       "      <td>-7.38407</td>\n",
       "      <td>3390.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-12-30-03</td>\n",
       "      <td>C0000002</td>\n",
       "      <td>C01</td>\n",
       "      <td>268--486--2021-12-30-03</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       nombre    codigo   XO   XA  longitud  latitud  altura  dato  90%  \\\n",
       "0      COSPAN  472D4658  278  489 -78.54106 -7.42856  2423.0   0.0  0.1   \n",
       "1  CASCABAMBA  472C92CA  268  486 -78.72682 -7.38407  3390.0   0.0  0.1   \n",
       "\n",
       "     99%  75%          fecha      flag flagV2                   imagen  clase  \n",
       "0  2.747  0.0  2021-12-03-13  C0000002    C01  278--489--2021-12-03-13      1  \n",
       "1  2.100  0.0  2021-12-30-03  C0000002    C01  268--486--2021-12-30-03      1  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtenemos un dataset de pruebas    \n",
    "pruebas    = 0.01\n",
    "validacion = 0.05\n",
    "\n",
    "_dsTrain, _dsTest = obtenerDSPruebas(dsCompleto, pruebas) \n",
    "_dsTrain, _dsValid = obtenerDSPruebas(_dsTrain, validacion)\n",
    "\n",
    "\n",
    "_dsTest.to_csv(f'{path_base}/Archivos/Dataset/Pruebas/{p_dataset[\"nameDS\"]}PruebasDS.csv')\n",
    "_dsValid.to_csv(f'{path_base}/Archivos/Dataset/{p_dataset[\"nameDS\"]}ValidDS.csv')\n",
    "_dsTrain.to_csv(f'{path_base}/Archivos/Dataset/{p_dataset[\"nameDS\"]}TrainDS.csv')\n",
    "\n",
    "\n",
    "print(f'TRAIN   : {len(_dsTrain)}')\n",
    "print(f'VALID   : {len(_dsValid)}')\n",
    "print(f'PRUEBAS : {len(_dsTest)}')\n",
    "\n",
    "#Agregamos el umbral al dataset\n",
    "_dsTrain.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d6d2e317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSeparamos el dataset de entrenamiento para balancear las clases\\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Separamos el dataset de entrenamiento para balancear las clases\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "96e5d09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyDA(df, columna='clase', DA=3):\n",
    "    df['DA'] = 0\n",
    "    dfs = [df[df[columna]==0], df[df[columna]==1]]    \n",
    "    posM = 0 if len(dfs[0])>len(dfs[1]) else 1\n",
    "    \n",
    "    # Clase minoritaria = M^1\n",
    "    # Se va aumentar lm\n",
    "    dfMenor0 = dfs[posM^1].copy()\n",
    "    \n",
    "    dfMenor1 = dfs[posM^1].copy()\n",
    "    dfMenor1['DA'] = 1\n",
    "    \n",
    "    dfMenor2 = dfs[posM^1].copy()\n",
    "    dfMenor2['DA'] = 2\n",
    "    \n",
    "    dfMenor3 = dfs[posM^1].copy()\n",
    "    dfMenor3['DA'] = 3\n",
    "    \n",
    "     \n",
    "    dfMenos = [dfMenor0,dfMenor1,dfMenor2,dfMenor3,dfs[posM]]\n",
    "    train = pd.concat(dfMenos)    \n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d377ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDataset(_dataset, columna, path_base, newDir='Clasificacion', DA = 3):    \n",
    "    if DA:\n",
    "        _dataset = applyDA(_dataset, 'clase', 3)\n",
    "        \n",
    "    dfs = [_dataset[_dataset[columna]==0], _dataset[_dataset[columna]==1]]    \n",
    "    posM = 0 if len(dfs[0])>len(dfs[1]) else 1\n",
    "    lM = len(dfs[posM])\n",
    "    lm = len(dfs[posM^1])\n",
    "    \n",
    "    try:\n",
    "        path = os.path.join(f'{path_base}/Archivos/Dataset/', newDir)\n",
    "        os.mkdir(path)        \n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    except:\n",
    "        print(f\"No se pudo crear la carpeta {newDir}\")\n",
    "    \n",
    "    \n",
    "    n = int(lM/lm) + (lM % lm > 0)    \n",
    "    splitted = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        if i == (n-1):\n",
    "            tempDF1 = dfs[posM][i*lm:i*lm+lm]\n",
    "            tempDF2 = dfs[posM][0:i*lm].sample(n=(lm-len(tempDF1)))            \n",
    "            splitted.append(pd.concat([tempDF1,tempDF2,dfs[posM^1]], ignore_index=True))            \n",
    "        else:\n",
    "            tempDF = dfs[posM][i*lm:i*lm+lm]            \n",
    "            splitted.append(pd.concat([tempDF,dfs[posM^1]], ignore_index=True))\n",
    "            \n",
    "    \n",
    "    i = 0\n",
    "    for df in splitted:        \n",
    "        df.to_csv(f'{path_base}/Archivos/Dataset/{newDir}/CLASE_TrainDS_{i}.csv')\n",
    "        i+=1\n",
    "    \n",
    "    return splitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f66b1486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#_dsTrain = applyDA(_dsTrain, 'clase', 3)\n",
    "#_dsTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3e6efd67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3., nan])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dsTrain['DA'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "00503fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "splittedDF = splitDataset(_dsTrain,'clase', path_base,'ClasificacionDA', DA= 3)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd005292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
