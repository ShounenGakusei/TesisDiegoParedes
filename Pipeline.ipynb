{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec8aa67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d0bc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b60c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d5be9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17734132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge cartopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "655158d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install netcdf4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43a709e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a38401a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbe66ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manejo de Datos\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Imagenes satelitales\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "from netCDF4 import Dataset, num2date\n",
    "\n",
    "#Machine learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#Librerias estandar (Extras)\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45d432e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda uninstall cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f54e6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ff0786b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17731649908387155228\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a24f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#El modelo solo considera en input_shape(x,x,1), el 1 se puede cambiar para abarcar mas canales de imagenes satelitales\n",
    "def crearModelo(W,H,dim, output):\n",
    "    print(f\"Se creo un modelo con input ({W,H,dim}) y output({output})\")\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(W, H, dim)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(output))\n",
    "    \n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "05306124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se le da un tensor de 4 dimensiones\n",
    "#[0] =  dato de precipitacion\n",
    "#[1] = Punto de la estacion (Longitud)\n",
    "#[2] = Punto de la estacion (Latitud)\n",
    "#[3] = Fecha (a침o-mes-dia-hora)\n",
    "\n",
    "#Devuelve x,y\n",
    "#X = Dato de precipitacion\n",
    "#Y = np.Array de las matrices de colores de cada producto en products (C08,C07 o C13)\n",
    "def leerImagenArea(tensor, umbral, path_base,margen,products): \n",
    "    \n",
    "    \"\"\"\n",
    "    -----------------------------------------------------------------------------------    \n",
    "    !!!!!VERIFICAR QUE LA HORA DE LA IMAGEN SATELITAL SEA IGUAL A LA HORA PERU!!!!!!!!\n",
    "    -----------------------------------------------------------------------------------\n",
    "    \n",
    "    Los archivos se deben encontrar en carpetas ordenadas : ../GOES/{producto}/{a침o}/{mes}/{ARCHIVO}.nc\n",
    "    ARCHIVO = G16_{producto}_Cyl_{a침o}{mes}{dia}-{hora}00.nc'\n",
    "    \n",
    "    EJEMPLO : path_base + GOES/C8/2019/02/G16_C08_Cyl_20190210-1600.nc\n",
    "    \"\"\"\n",
    "    \n",
    "    #Se define por defecto el path base - (Temporal)\n",
    "    #path_base  =  'C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/'\n",
    "    #path_base  =  '../GOES'\n",
    "    \n",
    "    try:\n",
    "        #Fecha = 2019 01 05 22\n",
    "        fecha = str(tensor.numpy()[3].decode('UTF-8'))\n",
    "        year,month,day,hour = fecha.split('-')\n",
    "        \n",
    "   \n",
    "    except:\n",
    "        print(\"No se pudo leer la fecha\")\n",
    "        print(tensor.numpy()[3].decode('UTF-8'))\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "    #El ancho y alto sera el margen que se dara desde el punto de origen (estacion)\n",
    "    #Esta en decimales (1 decimal == 100Km) - (Temporal)\n",
    "    alto= margen[0]\n",
    "    ancho= margen[1]\n",
    "    \n",
    "    \n",
    "    #Se define el producto \n",
    "    mapaArrays = []\n",
    "    for product in products:    \n",
    "        origen = [float(tensor.numpy()[1].decode('UTF-8')),float(tensor.numpy()[2].decode('UTF-8'))]    \n",
    "        filename = f'{path_base}/{product}/{year}/{month}/G16_{product}_Cyl_{year}{month}{day}-{hour}00.nc'    \n",
    "        try:\n",
    "            ds = Dataset(filename)\n",
    "        except:\n",
    "            print(\"No se pudo leer el archivo\")\n",
    "            print(filename)\n",
    "            return -1\n",
    "\n",
    "        # convierte el tiempo de formato numerico a formato fecha y hora de python\n",
    "        #date = num2date(ds.variables['time'][:], ds.variables['time'].units, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
    "\n",
    "        # convierte el formato de la variable de Int16 a Float32 y guarda el resultado\n",
    "        field = ds.variables['CMI'][:].data.astype(np.float32)/100.0\n",
    "\n",
    "        # obtiene las coordenadas de los pixeles\n",
    "        lon = ds.variables['longitude'][:].data\n",
    "        lat = ds.variables['latitude'][:].data    \n",
    "\n",
    "        #Se define el margen para recortar la imagen satelital\n",
    "        maxLon=origen[0]+ancho\n",
    "        minLon=origen[0]-ancho\n",
    "        maxLat=origen[1]+alto\n",
    "        minLat=origen[1]-alto\n",
    "\n",
    "        #Booleanos que ayudar치n a buscar el margen\n",
    "        altoMin = False\n",
    "        altoMax = False\n",
    "\n",
    "\n",
    "        #Inicializamos los \"indices\"\n",
    "        lom = 0\n",
    "        loM = 0\n",
    "        lam = 0\n",
    "        laM = 0\n",
    "\n",
    "        \"\"\"\n",
    "        Tener en cuenta que el arreglo de longitudes (lon) esta ordenado de manera creciente,\n",
    "        mientras que el de latitudes (lat) esta de manera decreciente\n",
    "        \"\"\"    \n",
    "        for i in range(0,len(lon)):\n",
    "            if lon[i]>=minLon and not altoMin:\n",
    "                altoMin = True\n",
    "                lom = i\n",
    "            if lon[i]<=maxLon:\n",
    "                loM = i\n",
    "\n",
    "        for j in range(0,len(lat)):\n",
    "            if lat[j]>=minLat:    \n",
    "                laM = j\n",
    "            if lat[j]<=maxLat and not altoMax:\n",
    "                altoMax = True\n",
    "                lam = j   \n",
    "                \n",
    "        mapaArrays.append(field[lam:laM,lom:loM])\n",
    "    \n",
    "    dato = float(tensor.numpy()[0].decode('UTF-8'))\n",
    "    if dato > umbral:\n",
    "        dato  = 1\n",
    "    else: \n",
    "        dato = 0   \n",
    "    \n",
    "    if len(products) == 1:\n",
    "        return mapaArrays[0], dato\n",
    "    \n",
    "    return np.dstack(mapaArrays), dato\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3f03320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Devuelve una lista con lo indices que no se encontraron lso archivos y el producto\n",
    "#Servira para ver si se teinen todas las imagenes necesarias para el entrenamiento\n",
    "def comprobarFile(df,products,path_base):        \n",
    "    no_index = []\n",
    "    no_product = []\n",
    "    for i in df.index:       \n",
    "        year,month,day,hour = df['fecha'][i].split('-')\n",
    "        tmpProduct = []        \n",
    "        for p in products:\n",
    "            filename = f'{path_base}/{p}/{year}/{month}/G16_{p}_Cyl_{year}{month}{day}-{hour}00.nc'       \n",
    "            existe = os.path.exists(filename)\n",
    "            if not existe:\n",
    "                tmpProduct.append(p)\n",
    "        if len(tmpProduct)>0:\n",
    "            no_index.append(i)                \n",
    "            no_product.append(tmpProduct)\n",
    "    \n",
    "    df = df.drop(index=no_index)\n",
    "    print(f'{len(no_index)} datos eliminados: No se encontraron los archivos de imagenes satelitales')\n",
    "    return df , (no_index,no_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "036bd060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenerDatos(filename):\n",
    "    pdata = pd.read_csv(filename) \n",
    "    \n",
    "    #Seleccionamos solo las columnas necesarias : precipitacion, Estacion (Longitud), Estacion (Latitud), Fecha (a침o-mes-dia-hora)\n",
    "    pdataX = pdata.loc[:, ['dato','longitud', 'latitud', 'fecha']]\n",
    "\n",
    "    #Quitamos los valores NA\n",
    "    pdataX = pdataX[pdataX['dato'].notna()]\n",
    "\n",
    "    #Definimos un solo tipo (str) pora asi poder convertirlo a tensor\n",
    "    pdataX = pdataX.astype({\"dato\":str,\"longitud\":str, \"latitud\":  str, \"fecha\": str})\n",
    "                \n",
    "    #Barajeamos los datos\n",
    "    pdataX = shuffle(pdataX)\n",
    "    \n",
    "    print(f'{len(pdataX)} datos leidos')\n",
    "    return pdataX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "24366046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyDataset(dataset,umbral, path_base,margen,products):\n",
    "    x = []\n",
    "    y = []\n",
    "    i,j = 0.0 , []    \n",
    "    for dato in dataset:       \n",
    "        i,j =  leerImagenArea(dato,umbral, path_base,margen,products)\n",
    "        x.append(i)\n",
    "        y.append(j)\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    return x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fd5e08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def train_step(x,y,model,optimizer,loss_fn,train_acc_metric):    \n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0cc28e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def test_step(x,y,model,val_acc_metric):      \n",
    "    val_logits = model(x, training=False)\n",
    "    val_acc_metric.update_state(y, val_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7a77f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenamiento(datasetList,umbral,model,path_base,margen,products, batch_size,train_size,epocas=2):  \n",
    "    \n",
    "    #Dividmos el dataset (Entrenamient - Validacion)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices(datasetList[:train_size])           \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices(datasetList[train_size:])\n",
    "\n",
    "    #Divimos en batchs los datasets\n",
    "    train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
    "    val_dataset = val_dataset.batch(batch_size)\n",
    "    \n",
    "\n",
    "    #Definimos Variables del modelo (optmizador, funcion de loss, metricas, etc)\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)        \n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "    val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "        \n",
    "    #Entrenamos el modelo\n",
    "    for epoch in range(epocas):    \n",
    "        print(\"\\nComienzo de la epoca %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        for step, (datos) in enumerate(train_dataset):            \n",
    "            #Obtenmos el verdadero dataset (valor, matriz) del batch X\n",
    "            x_train_batch, y_train_batch =  xyDataset(datos,umbral, path_base,margen,products)  \n",
    "            \n",
    "            #Se obtiene el valor de perdida para el batch X\n",
    "            loss_value = train_step(x_train_batch,y_train_batch,model,optimizer,loss_fn,train_acc_metric)\n",
    "                        \n",
    "            #Log every 200 batches.\n",
    "            if step % 5 == 0:\n",
    "                print(\n",
    "                    \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                    % (step, float(loss_value))\n",
    "                )\n",
    "                print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
    "        \n",
    "        #Imprimimos y reiniciamos las metricas para una epoca\n",
    "        train_acc = train_acc_metric.result()\n",
    "        print(\"Training acc over epoch: %.4f\" % (float(train_acc),))        \n",
    "        train_acc_metric.reset_states()\n",
    "\n",
    "        #Usamos el dataset de validacion para la validacion\n",
    "        for (datos) in val_dataset:\n",
    "            #Verdadero dataset\n",
    "            x_val_batch, y_val_batch =  xyDataset(datos,umbral, path_base,margen,products)\n",
    "\n",
    "            #Evaluamos\n",
    "            test_step(x_val_batch, y_val_batch, model,val_acc_metric)\n",
    "\n",
    "        #Imprimimos y reinciamos\n",
    "        val_acc = val_acc_metric.result()\n",
    "        print(\"Validation acc: %.4f\" % (float(val_acc),))    \n",
    "        val_acc_metric.reset_states()\n",
    "        print(\"Time taken: %.2fs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d01fc2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path_base debe ser completo, se usar치 para comprobar si existen las imagenes satelitales descargadas\n",
    "path_base = 'C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/GOES/'\n",
    "\n",
    "#Productos de las imagenes satelitales (C08, C07 o C13, C02)\n",
    "products = ['C08']\n",
    "\n",
    "#El margen servira para recortar la imagen [alto, ancho] desde el punto de origen (estacion), esta en decimales\n",
    "margen = [1,1]\n",
    "\n",
    "#Batach -1 = entrenara con todo el dataset al mismo tiempo\n",
    "batch = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3db4f3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336720 datos leidos\n"
     ]
    }
   ],
   "source": [
    "#Leemos los datos del archivo\n",
    "#Archivo de prueba contiene datos del 2019 del mes 01 y 02\n",
    "dfOrignial = obtenerDatos('pruebasV2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d228764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1444 datos eliminados: No se encontraron los archivos de imagenes satelitales\n"
     ]
    }
   ],
   "source": [
    "#Comprobamos si existen las imagenes/produtos por cada dato,\n",
    "#caso contrario los borra de la lista\n",
    "dfVerificado, (no_i,no_p) = comprobarFile(dfOrignial,products,path_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e91a3630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "['4.5', '-76.62455', '-11.13916', '2019-02-23-17']\n"
     ]
    }
   ],
   "source": [
    "#Seleccionamos algunos para las pruebas\n",
    "df = dfVerificado[0:1000]\n",
    "datasetList = df.values.tolist()\n",
    "\n",
    "#-Visualizacion\n",
    "print(len(datasetList))\n",
    "print(datasetList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a37aa890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se creo un modelo con input ((110, 110, 1)) y output(2)\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_9 (Conv2D)           (None, 108, 108, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 54, 54, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 52, 52, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 26, 26, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 24, 24, 64)        36928     \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 36864)             0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                2359360   \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,415,234\n",
      "Trainable params: 2,415,234\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Creamos el modelo\n",
    "dimOutput = 2\n",
    "tempTensor =  tf.constant(datasetList[0])\n",
    "imagenT, datoT = leerImagenArea(tempTensor, path_base,margen,products)\n",
    "modelo = crearModelo(imagenT.shape[0],imagenT.shape[1],len(products),dimOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "02c47c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comienzo de la epoca 0\n",
      "Training loss (for one batch) at step 0: 1.7938\n",
      "Seen so far: 100 samples\n",
      "Training loss (for one batch) at step 5: 0.2696\n",
      "Seen so far: 600 samples\n",
      "Training acc over epoch: 0.8575\n",
      "Validation acc: 0.9750\n",
      "Time taken: 19.19s\n",
      "\n",
      "Comienzo de la epoca 1\n",
      "Training loss (for one batch) at step 0: 0.1679\n",
      "Seen so far: 100 samples\n",
      "Training loss (for one batch) at step 5: 0.2543\n",
      "Seen so far: 600 samples\n",
      "Training acc over epoch: 0.8525\n",
      "Validation acc: 0.9750\n",
      "Time taken: 19.59s\n",
      "\n",
      "Comienzo de la epoca 2\n",
      "Training loss (for one batch) at step 0: 0.2959\n",
      "Seen so far: 100 samples\n",
      "Training loss (for one batch) at step 5: 0.7532\n",
      "Seen so far: 600 samples\n",
      "Training acc over epoch: 0.9725\n",
      "Validation acc: 0.0250\n",
      "Time taken: 19.93s\n",
      "\n",
      "Comienzo de la epoca 3\n",
      "Training loss (for one batch) at step 0: 0.9669\n",
      "Seen so far: 100 samples\n",
      "Training loss (for one batch) at step 5: 0.6428\n",
      "Seen so far: 600 samples\n",
      "Training acc over epoch: 0.8550\n",
      "Validation acc: 0.9750\n",
      "Time taken: 23.08s\n"
     ]
    }
   ],
   "source": [
    "#POR EL MOMENTO el batch_size debe poder dividirse entre la cantidad total del dataset (no residuo) \n",
    "batch_size = 100\n",
    "\n",
    "#El Umbral esta en mm/h, igual que el dataset. Si supera este umbral se considera 1 (Extremo) sino 0 (no extremo)\n",
    "umbral = 2.0\n",
    "\n",
    "train_size = int(len(datasetList)*0.8)\n",
    "epocas = 4\n",
    "\n",
    "#Entrenamos con TODO el dataset              \n",
    "entrenamiento(datasetList,umbral,modelo,path_base,margen,products, batch_size,train_size, epocas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2d7de8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamos con BATCH\n",
    "#entrenamiento(modelo,datasetList,path_base,margen,products, 64 ,epocas=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744e5b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
