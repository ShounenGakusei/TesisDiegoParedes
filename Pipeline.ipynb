{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbe66ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from netCDF4 import Dataset, num2date\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cf\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import re\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a24f272",
   "metadata": {},
   "outputs": [],
   "source": [
    "#El modelo solo considera en input_shape(x,x,1), el 1 se puede cambiar para abarcar mas canales de imagenes satelitales\n",
    "def crearModelo(inputLen):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(110, 110, inputLen)))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dense(40))\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "#testModel = crearModelo(1)\n",
    "#testModel.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05306124",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se le da un tensor de 4 dimensiones\n",
    "#[0] =  dato de precipitacion\n",
    "#[1] = Punto de la estacion (Longitud)\n",
    "#[2] = Punto de la estacion (Latitud)\n",
    "#[3] = Fecha (año-mes-dia-hora)\n",
    "\n",
    "#Devuelve x,y\n",
    "#X = Dato de precipitacion\n",
    "#Y = np.Array de las matrices de colores de cada producto en products (C08,C07 o C13)\n",
    "def leerImagenArea(tensor, path_base,margen,products): \n",
    "    \n",
    "    \"\"\"\n",
    "    -----------------------------------------------------------------------------------    \n",
    "    !!!!!VERIFICAR QUE LA HORA DE LA IMAGEN SATELITAL SEA IGUAL A LA HORA PERU!!!!!!!!\n",
    "    -----------------------------------------------------------------------------------\n",
    "    \n",
    "    Los archivos se deben encontrar en carpetas ordenadas : ../GOES/{producto}/{año}/{mes}/{ARCHIVO}.nc\n",
    "    ARCHIVO = G16_{producto}_Cyl_{año}{mes}{dia}-{hora}00.nc'\n",
    "    \n",
    "    EJEMPLO : path_base + GOES/C8/2019/02/G16_C08_Cyl_20190210-1600.nc\n",
    "    \"\"\"\n",
    "    \n",
    "    #Se define por defecto el path base - (Temporal)\n",
    "    #path_base  =  'C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/'\n",
    "    #path_base  =  '../GOES'\n",
    "    \n",
    "    try:\n",
    "        #Fecha = 2019 01 05 22\n",
    "        fecha = str(tensor.numpy()[3].decode('UTF-8'))\n",
    "        year,month,day,hour = fecha.split('-')\n",
    "        \n",
    "   \n",
    "    except:\n",
    "        print(\"No se pudo leer la fecha\")\n",
    "        print(tensor.numpy()[3].decode('UTF-8'))\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "    #El ancho y alto sera el margen que se dara desde el punto de origen (estacion)\n",
    "    #Esta en grados (1 grado == 100Km) - (Temporal)\n",
    "    alto= margen[0]\n",
    "    ancho= margen[1]\n",
    "    \n",
    "    \n",
    "    #Se define el producto \n",
    "    mapaArrays = []\n",
    "    for product in products:    \n",
    "        origen = [float(tensor.numpy()[1].decode('UTF-8')),float(tensor.numpy()[2].decode('UTF-8'))]    \n",
    "        filename = f'{path_base}/{product}/{year}/{month}/G16_{product}_Cyl_{year}{month}{day}-{hour}00.nc'    \n",
    "        try:\n",
    "            ds = Dataset(filename)\n",
    "        except:\n",
    "            print(\"No se pudo leer el archivo\")\n",
    "            print(filename)\n",
    "            return -1\n",
    "\n",
    "        # convierte el tiempo de formato numerico a formato fecha y hora de python\n",
    "        #date = num2date(ds.variables['time'][:], ds.variables['time'].units, only_use_cftime_datetimes=False, only_use_python_datetimes=True)\n",
    "\n",
    "        # convierte el formato de la variable de Int16 a Float32 y guarda el resultado\n",
    "        field = ds.variables['CMI'][:].data.astype(np.float32)/100.0\n",
    "\n",
    "        # obtiene las coordenadas de los pixeles\n",
    "        lon = ds.variables['longitude'][:].data\n",
    "        lat = ds.variables['latitude'][:].data    \n",
    "\n",
    "        #Se define el margen para recortar la imagen satelital\n",
    "        maxLon=origen[0]+ancho\n",
    "        minLon=origen[0]-ancho\n",
    "        maxLat=origen[1]+alto\n",
    "        minLat=origen[1]-alto\n",
    "\n",
    "        #Booleanos que ayudarán a buscar el margen\n",
    "        altoMin = False\n",
    "        altoMax = False\n",
    "\n",
    "\n",
    "        #Inicializamos los \"indices\"\n",
    "        lom = 0\n",
    "        loM = 0\n",
    "        lam = 0\n",
    "        laM = 0\n",
    "\n",
    "        \"\"\"\n",
    "        Tener en cuenta que el arreglo de longitudes (lon) esta ordenado de manera creciente,\n",
    "        mientras que el de latitudes (lat) esta de manera decreciente\n",
    "        \"\"\"    \n",
    "        for i in range(0,len(lon)):\n",
    "            if lon[i]>=minLon and not altoMin:\n",
    "                altoMin = True\n",
    "                lom = i\n",
    "            if lon[i]<=maxLon:\n",
    "                loM = i\n",
    "\n",
    "        for j in range(0,len(lat)):\n",
    "            if lat[j]>=minLat:    \n",
    "                laM = j\n",
    "            if lat[j]<=maxLat and not altoMax:\n",
    "                altoMax = True\n",
    "                lam = j   \n",
    "                \n",
    "        mapaArrays.append(field[lam:laM,lom:loM])\n",
    "    \n",
    "    if len(products) == 1:\n",
    "        return mapaArrays[0], float(tensor.numpy()[0].decode('UTF-8'))\n",
    "    \n",
    "    return np.dstack(mapaArrays), float(tensor.numpy()[0].decode('UTF-8'))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3f03320",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Devuelve una lista con lo indices que no se encontraron lso archivos y el producto\n",
    "#Servira para ver si se teinen todas las imagenes necesarias para el entrenamiento\n",
    "def comprobarFile(df,products,path_base):        \n",
    "    no_index = []\n",
    "    no_product = []\n",
    "    for i in df.index:       \n",
    "        year,month,day,hour = df['fecha'][i].split('-')\n",
    "        tmpProduct = []        \n",
    "        for p in products:\n",
    "            filename = f'{path_base}/{p}/{year}/{month}/G16_{p}_Cyl_{year}{month}{day}-{hour}00.nc'       \n",
    "            existe = os.path.exists(filename)\n",
    "            if not existe:\n",
    "                tmpProduct.append(p)\n",
    "        if len(tmpProduct)>0:\n",
    "            no_index.append(i)                \n",
    "            no_product.append(tmpProduct)\n",
    "    \n",
    "    df = df.drop(index=no_index)\n",
    "    print(f'{len(no_index)} datos eliminados: No se encontraron los archivos de imagenes satelitales')\n",
    "    return df , (no_index,no_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "036bd060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtenerDatos(filename):\n",
    "    pdata = pd.read_csv(filename) \n",
    "    \n",
    "    #Seleccionamos solo las columnas necesarias : precipitacion, Estacion (Longitud), Estacion (Latitud), Fecha (año-mes-dia-hora)\n",
    "    pdataX = pdata.loc[:, ['dato','longitud', 'latitud', 'fecha']]\n",
    "\n",
    "    #Quitamos los valores NA\n",
    "    pdataX = pdataX[pdataX['dato'].notna()]\n",
    "\n",
    "    #Definimos un solo tipo (str) pora asi poder convertirlo a tensor\n",
    "    pdataX = pdataX.astype({\"dato\":str,\"longitud\":str, \"latitud\":  str, \"fecha\": str})\n",
    "                \n",
    "    #Barajeamos los datos\n",
    "    pdataX = shuffle(pdataX)\n",
    "    \n",
    "    print(f'{len(pdataX)} datos leidos')\n",
    "    return pdataX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bb029c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyDataset(dataset, path_base,margen,products):\n",
    "    x = []\n",
    "    y = []\n",
    "    i,j = 0.0 , []    \n",
    "    for dato in dataset:       \n",
    "        i,j =  leerImagenArea(dato, path_base,margen,products)\n",
    "        x.append(i)\n",
    "        y.append(j)\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "    return x,y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fd5e08a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "\n",
    "def train_step(x,y,model,optimizer,loss_fn,train_acc_metric):    \n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0cc28e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "\n",
    "def test_step(x,y,model,val_acc_metric):      \n",
    "    val_logits = model(x, training=False)\n",
    "    val_acc_metric.update_state(y, val_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "74d1115b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrenamiento(model,datasetList,path_base,margen,products, batch_size,epocas=2):\n",
    "    #Definimos el optimizador\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    \n",
    "    #Definimos la funcion de peridida\n",
    "    loss_fn = keras.losses.MeanAbsoluteError()\n",
    "    \n",
    "    #Definimos las metricas a evaluar\n",
    "    train_acc_metric = keras.metrics.MeanSquaredError()\n",
    "    val_acc_metric = keras.metrics.MeanSquaredError()\n",
    "       \n",
    "    \n",
    "    for epoch in range(epocas):\n",
    "        print(\"\\nComienzo de la epoca %d\" % (epoch,))\n",
    "        start_time = time.time()\n",
    "        \n",
    "        #Procesamos y separamos el dataset en grupos para el entrenamiento\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(datasetList)\n",
    "        train_size = int(len(datasetList)*0.8)\n",
    "        #train_dataset = dataset.take(train_size)\n",
    "        #val_dataset = dataset.skip(train_size)\n",
    "\n",
    "        #Dataset de entrenamiento\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(datasetList[:train_size])       \n",
    "        \n",
    "\n",
    "        #Dataset de validacion\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices(datasetList[train_size:])\n",
    "        \n",
    "        \n",
    "        ##################################\n",
    "        #ENTREMAINETO CON BATCH\n",
    "        ##################################\n",
    "        if batch_size != -1:\n",
    "            #print(\"No implementado aun\")\n",
    "            #return\n",
    "            train_dataset = train_dataset.batch(batch_size)\n",
    "            val_dataset = val_dataset.batch(batch_size)\n",
    "            \n",
    "            #Iterate over the batches of the dataset.\n",
    "            for step, (datos) in enumerate(train_dataset):\n",
    "                #Obtenmos X,Y\n",
    "                x_train_batch, y_train_batch =  xyDataset(datos, path_base,margen,products)\n",
    "                \n",
    "                #Entrenamos\n",
    "                loss_value = train_step(x_train_batch, y_train_batch, model,optimizer,loss_fn,train_acc_metric)\n",
    "                \n",
    "                #Imprimir log\n",
    "                if step % 200 == 0:\n",
    "                    print(\n",
    "                        \"Training loss (para un batch) en el paso %d: %.4f\"\n",
    "                        % (step, float(loss_value))\n",
    "                    )\n",
    "                    print(\"Datos procesados: %d samples\" % ((step + 1) * batch_size))\n",
    "\n",
    "            #Imprimir metricas al final de cada epoca\n",
    "            train_acc = train_acc_metric.result()\n",
    "            print(\"Training acc en la epoca: %.4f\" % (float(train_acc),))\n",
    "\n",
    "            #Reseteamos al metrica\n",
    "            train_acc_metric.reset_states()\n",
    "\n",
    "            # Run a validation loop at the end of each epoch.\n",
    "            for (datos) in val_dataset:\n",
    "                #Obtenmos X,Y\n",
    "                x_val_batch, y_val_batch =  xyDataset(datos, path_base,margen,products)\n",
    "                \n",
    "                #Evaluamos\n",
    "                test_step(x_val_batch, y_val_batch, model,val_acc_metric)\n",
    "\n",
    "            #Imrpimimos resultados de valdiacion    \n",
    "            val_acc = val_acc_metric.result()\n",
    "            val_acc_metric.reset_states()\n",
    "            print(\"Validation acc: %.4f\" % (float(val_acc),))    \n",
    "            print(\"Time taken: %.2fs\" % (time.time() - start_time))    \n",
    "            \n",
    "        ##################################\n",
    "        #ENTREMAINETO CON TODO EL DATASET\n",
    "        ##################################\n",
    "        else:\n",
    "            #Obtenemos dataset de etrenamiento\n",
    "            x_train, y_train = xyDataset(train_dataset, path_base,margen,products)\n",
    "            \n",
    "                        \n",
    "            #Obtenemos dataset de validacion      \n",
    "            x_val, y_val = xyDataset(val_dataset, path_base,margen,products)    \n",
    "            \n",
    "            \n",
    "            #Entrenamos el modelo\n",
    "            model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.MeanAbsoluteError(),\n",
    "              metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "            history = model.fit(x=x_train, y= y_train, validation_data=(x_val,y_val))\n",
    "            \n",
    "                 \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d01fc2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path_base debe ser completo, se usará para comprobar si existen las imagenes satelitales descargadas\n",
    "path_base = 'C:/Users/Shounen/Desktop/Ciclo XI/Tesis 2/GOES/'\n",
    "\n",
    "#Productos de las imagenes satelitales (C08, C07 o C13)\n",
    "products = ['C08','C07']\n",
    "\n",
    "#El margen servira para recortar la imagen [alto, ancho] desde el punto de origen (estacion)\n",
    "margen = [1,1]\n",
    "\n",
    "#Batach -1 = entrenara con todo el dataset al mismo tiempo\n",
    "batch = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a37aa890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 108, 108, 32)      608       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 54, 54, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 52, 52, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 26, 26, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 64)        36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 36864)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                2359360   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 40)                2600      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,417,992\n",
      "Trainable params: 2,417,992\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Creamos el modelo\n",
    "modelo = crearModelo(len(products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3db4f3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "336720 datos leidos\n"
     ]
    }
   ],
   "source": [
    "#Leemos los datos del archivo\n",
    "#Archivo de prueba contiene datos del 2019 del mes 01 y 02\n",
    "df = obtenerDatos('pruebasV2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d228764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273800 datos eliminados: No se encontraron los archivos de imagenes satelitales\n"
     ]
    }
   ],
   "source": [
    "#Comprobamos si existen las imagenes/produtos por cada dato,\n",
    "#caso contrario los borra de la lista\n",
    "df, (no_i,no_p) = comprobarFile(df,products,path_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e91a3630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "['0.0', '-70.6775', '-15.74562', '2019-02-27-00']\n"
     ]
    }
   ],
   "source": [
    "#Seleccionamos algunos para las pruebas\n",
    "df = df[0:200]\n",
    "datasetList = df.values.tolist()\n",
    "\n",
    "#-Visualizacion\n",
    "print(len(datasetList))\n",
    "print(datasetList[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23e1fde2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0.0', '-70.6775', '-15.74562', '2019-02-27-00'],\n",
       " ['0.1', '-74.2642', '-14.5423', '2019-02-27-16'],\n",
       " ['0.0', '-76.5909', '-9.87895', '2019-01-31-06'],\n",
       " ['0.0', '-71.01823', '-13.92214', '2019-01-29-12'],\n",
       " ['0.0', '-71.38955', '-15.21331', '2019-02-25-02'],\n",
       " ['0.0', '-73.27164', '-14.52014', '2019-02-26-08'],\n",
       " ['0.0', '-74.96083', '-9.38139', '2019-02-28-20'],\n",
       " ['0.1', '-76.37972', '-11.11333', '2019-01-28-01'],\n",
       " ['0.0', '-70.17122', '-15.47106', '2019-02-28-09'],\n",
       " ['0.0', '-78.05111', '-7.62167', '2019-02-28-07'],\n",
       " ['0.0', '-77.0432', '-12.07057', '2019-01-29-08'],\n",
       " ['0.0', '-78.82367', '-7.47987', '2019-01-28-17'],\n",
       " ['0.0', '-78.70694', '-7.57417', '2019-01-26-07'],\n",
       " ['0.0', '-78.72682', '-7.38407', '2019-01-29-18'],\n",
       " ['0.0', '-78.52363', '-6.67996', '2019-01-29-07'],\n",
       " ['0.0', '-77.44643', '-9.47683', '2019-01-31-13'],\n",
       " ['0.0', '-77.40607', '-9.85243', '2019-01-08-22'],\n",
       " ['0.0', '-79.69924', '-5.28985', '2019-02-26-21'],\n",
       " ['0.0', '-76.76952', '-11.38887', '2019-02-24-07'],\n",
       " ['0.1', '-71.11667', '-15.81667', '2019-01-30-08'],\n",
       " ['0.0', '-70.17213', '-17.47491', '2019-02-25-05'],\n",
       " ['0.0', '-76.60639', '-11.73361', '2019-02-27-09'],\n",
       " ['0.0', '-77.15139', '-11.77606', '2019-01-28-21'],\n",
       " ['0.0', '-76.52415', '-11.98311', '2019-02-28-13'],\n",
       " ['0.0', '-71.23669', '-14.23736', '2019-01-29-09'],\n",
       " ['0.0', '-71.72056', '-16.25944', '2019-02-27-05'],\n",
       " ['0.0', '-78.99989', '-5.14422', '2019-01-30-02'],\n",
       " ['0.0', '-70.04305', '-17.0788', '2019-02-27-15'],\n",
       " ['0.0', '-78.83801', '-7.21976', '2019-02-28-14'],\n",
       " ['0.0', '-78.17275', '-7.32254', '2019-01-30-17'],\n",
       " ['0.0', '-80.34703', '-4.93848', '2019-02-24-10'],\n",
       " ['0.0', '-76.77937', '-11.19005', '2019-02-28-22'],\n",
       " ['0.0', '-77.40607', '-9.85243', '2019-02-24-06'],\n",
       " ['1.1', '-69.62672', '-17.57994', '2019-01-29-15'],\n",
       " ['0.2', '-79.69924', '-5.28985', '2019-02-26-03'],\n",
       " ['0.0', '-76.65514', '-11.23284', '2019-02-26-22'],\n",
       " ['0.0', '-79.42792', '-6.65489', '2019-02-24-10'],\n",
       " ['0.0', '-70.08281', '-13.88492', '2019-02-26-01'],\n",
       " ['0.0', '-77.91019', '-8.50942', '2019-02-26-19'],\n",
       " ['0.0', '-73.63833', '-14.04361', '2019-01-26-08'],\n",
       " ['0.0', '-72.15266', '-16.33565', '2019-02-26-18'],\n",
       " ['0.0', '-77.52504', '-10.07116', '2019-01-27-00'],\n",
       " ['0.0', '-75.64806', '-13.04889', '2019-01-31-19'],\n",
       " ['0.0', '-75.69028', '-11.39694', '2019-02-25-01'],\n",
       " ['0.1', '-72.82447', '-12.24986', '2019-01-31-13'],\n",
       " ['0.0', '-73.17481', '-13.99183', '2019-01-30-06'],\n",
       " ['0.0', '-76.15944', '-11.71053', '2019-01-30-01'],\n",
       " ['0.5', '-78.78432', '-6.54049', '2019-01-29-09'],\n",
       " ['0.0', '-76.77937', '-11.19005', '2019-01-26-09'],\n",
       " ['0.0', '-78.20828', '-9.43372', '2019-01-28-06'],\n",
       " ['0.0', '-76.37792', '-11.83913', '2019-01-29-06'],\n",
       " ['0.0', '-75.87215', '-12.3444', '2019-01-27-08'],\n",
       " ['0.0', '-74.51953', '-8.36225', '2019-02-23-15'],\n",
       " ['0.0', '-75.87215', '-12.3444', '2019-01-31-06'],\n",
       " ['0.3', '-70.17213', '-17.47491', '2019-01-30-01'],\n",
       " ['0.0', '-79.42792', '-6.65489', '2019-02-27-17'],\n",
       " ['0.0', '-76.63487', '-11.19827', '2019-02-24-01'],\n",
       " ['0.0', '-73.68839', '-13.63369', '2019-02-24-12'],\n",
       " ['0.0', '-70.93167', '-17.16917', '2019-02-25-07'],\n",
       " ['0.2', '-76.38294', '-11.342', '2019-01-27-18'],\n",
       " ['0.0', '-69.62592', '-16.08825', '2019-02-27-05'],\n",
       " ['0.0', '-79.22389', '-6.47906', '2019-02-28-02'],\n",
       " ['0.0', '-70.28831', '-17.11413', '2019-02-23-22'],\n",
       " ['0.0', '-78.67588', '-6.55405', '2019-02-28-19'],\n",
       " ['0.0', '-74.96083', '-9.38139', '2019-01-29-08'],\n",
       " ['1.1', '-76.56359', '-11.84696', '2019-02-25-11'],\n",
       " ['0.0', '-77.15679', '-10.14431', '2019-01-31-01'],\n",
       " ['0.0', '-78.69759', '-5.45077', '2019-01-26-11'],\n",
       " ['0.0', '-76.96417', '-12.02528', '2019-02-26-13'],\n",
       " ['0.0', '-75.25', '-13.83333', '2019-01-27-10'],\n",
       " ['0.0', '-71.23186', '-14.16994', '2019-01-28-13'],\n",
       " ['0.0', '-76.31834', '-6.58923', '2019-01-29-18'],\n",
       " ['0.0', '-78.52363', '-6.67996', '2019-02-24-13'],\n",
       " ['0.3', '-70.04305', '-17.0788', '2019-01-29-00'],\n",
       " ['0.1', '-77.20234', '-9.31543', '2019-02-27-16'],\n",
       " ['0.1', '-76.62603', '-11.45481', '2019-02-23-21'],\n",
       " ['0.0', '-78.05111', '-7.62167', '2019-01-26-12'],\n",
       " ['0.0', '-79.71077', '-4.63776', '2019-01-26-11'],\n",
       " ['0.0', '-78.52363', '-6.67996', '2019-01-30-06'],\n",
       " ['0.0', '-76.15111', '-9.84694', '2019-02-26-06'],\n",
       " ['0.2', '-76.05328', '-11.73986', '2019-02-28-16'],\n",
       " ['2.2', '-71.41481', '-16.05542', '2019-01-27-15'],\n",
       " ['0.0', '-76.5909', '-9.87895', '2019-01-29-18'],\n",
       " ['0.0', '-71.11667', '-15.81667', '2019-02-26-03'],\n",
       " ['0.0', '-72.54585', '-13.16655', '2019-01-27-18'],\n",
       " ['0.0', '-77.91019', '-8.50942', '2019-02-26-18'],\n",
       " ['0.0', '-70.00949', '-17.23771', '2019-01-28-18'],\n",
       " ['0.0', '-76.60972', '-11.91417', '2019-01-28-21'],\n",
       " ['0.9', '-79.52542', '-5.56599', '2019-02-24-02'],\n",
       " ['2.2', '-71.60169', '-15.64163', '2019-01-29-16'],\n",
       " ['0.0', '-78.72682', '-7.38407', '2019-02-24-07'],\n",
       " ['0.0', '-77.15139', '-11.77606', '2019-02-26-22'],\n",
       " ['0.1', '-79.60535', '-5.40058', '2019-01-27-07'],\n",
       " ['0.0', '-74.2642', '-14.5423', '2019-02-27-09'],\n",
       " ['0.0', '-80.42964', '-3.55117', '2019-02-27-12'],\n",
       " ['0.0', '-76.02778', '-9.32', '2019-01-27-16'],\n",
       " ['0.0', '-75.98094', '-5.61622', '2019-02-28-06'],\n",
       " ['0.1', '-76.77937', '-11.19005', '2019-01-30-23'],\n",
       " ['0.0', '-74.95588', '-14.632', '2019-01-29-18'],\n",
       " ['0.0', '-77.44643', '-9.47683', '2019-01-29-12'],\n",
       " ['0.1', '-71.65124', '-15.18255', '2019-02-27-18'],\n",
       " ['0.0', '-70.93167', '-17.16917', '2019-01-31-12'],\n",
       " ['0.0', '-78.20828', '-9.43372', '2019-01-28-15'],\n",
       " ['0.0', '-69.62592', '-16.08825', '2019-01-01-00'],\n",
       " ['0.3', '-70.28831', '-17.11413', '2019-01-31-06'],\n",
       " ['0.0', '-78.30137', '-9.47982', '2019-02-27-08'],\n",
       " ['0.0', '-79.61317', '-4.74767', '2019-01-28-03'],\n",
       " ['0.3', '-79.22389', '-6.47906', '2019-02-23-18'],\n",
       " ['0.0', '-77.90148', '-7.9123', '2019-01-31-13'],\n",
       " ['0.0', '-80.505', '-3.99945', '2019-02-23-16'],\n",
       " ['0.0', '-77.39676', '-10.40312', '2019-01-31-10'],\n",
       " ['0.0', '-77.91019', '-8.50942', '2019-01-29-07'],\n",
       " ['0.0', '-79.94584', '-4.63439', '2019-01-26-22'],\n",
       " ['1.2', '-69.62672', '-17.57994', '2019-01-28-15'],\n",
       " ['0.2', '-80.65058', '-3.93911', '2019-01-28-02'],\n",
       " ['0.0', '-80.17296', '-5.1128', '2019-02-27-04'],\n",
       " ['0.0', '-77.18475', '-8.98039', '2019-02-28-02'],\n",
       " ['0.0', '-73.27164', '-14.52014', '2019-02-28-16'],\n",
       " ['0.0', '-75.3', '-13.48333', '2019-02-28-14'],\n",
       " ['11.7', '-80.65058', '-3.93911', '2019-02-24-03'],\n",
       " ['0.0', '-79.83761', '-5.34602', '2019-01-31-00'],\n",
       " ['0.0', '-76.60639', '-11.73361', '2019-01-31-08'],\n",
       " ['0.0', '-76.77937', '-11.19005', '2019-01-31-15'],\n",
       " ['0.2', '-78.55135', '-7.64333', '2019-01-29-05'],\n",
       " ['0.0', '-70.6775', '-15.74562', '2019-02-24-12'],\n",
       " ['0.0', '-76.31834', '-6.58923', '2019-01-28-07'],\n",
       " ['0.0', '-76.62455', '-11.13916', '2019-02-28-22'],\n",
       " ['0.0', '-76.4705', '-11.36111', '2019-02-28-17'],\n",
       " ['0.0', '-76.35011', '-11.57056', '2019-02-26-06'],\n",
       " ['0.0', '-69.94863', '-17.3692', '2019-01-27-06'],\n",
       " ['0.0', '-78.99989', '-5.14422', '2019-01-27-10'],\n",
       " ['0.0', '-70.75028', '-9.75028', '2019-02-27-19'],\n",
       " ['0.0', '-74.49886', '-7.89825', '2019-02-26-16'],\n",
       " ['0.0', '-77.17339', '-9.8986', '2019-01-30-08'],\n",
       " ['0.0', '-78.91262', '-6.58271', '2019-02-24-06'],\n",
       " ['0.0', '-79.97107', '-5.19465', '2019-01-30-11'],\n",
       " ['0.0', '-71.36446', '-12.8312', '2019-01-27-13'],\n",
       " ['0.0', '-78.72682', '-7.38407', '2019-01-31-15'],\n",
       " ['0.0', '-73.83612', '-5.04308', '2019-02-24-23'],\n",
       " ['0.0', '-77.00777', '-12.10863', '2019-02-27-13'],\n",
       " ['0.6', '-76.15556', '-11.84056', '2019-02-26-03'],\n",
       " ['0.0', '-70.59315', '-15.23587', '2019-01-27-20'],\n",
       " ['2.2', '-75.16667', '-14.03333', '2019-01-26-16'],\n",
       " ['0.1', '-77.40607', '-9.85243', '2019-01-30-01'],\n",
       " ['0.0', '-78.47729', '-7.7675', '2019-02-26-22'],\n",
       " ['0.0', '-78.30137', '-9.47982', '2019-02-26-02'],\n",
       " ['0.0', '-71.41481', '-16.05542', '2019-02-25-11'],\n",
       " ['0.0', '-76.65701', '-11.22343', '2019-02-24-01'],\n",
       " ['0.0', '-71.60169', '-15.64163', '2019-02-26-14'],\n",
       " ['0.0', '-78.72682', '-7.38407', '2019-02-24-23'],\n",
       " ['0.0', '-69.66186', '-17.30847', '2019-02-26-06'],\n",
       " ['0.0', '-70.51206', '-3.91125', '2019-01-28-04'],\n",
       " ['0.0', '-70.17213', '-17.47491', '2019-02-28-20'],\n",
       " ['0.0', '-70.17122', '-15.47106', '2019-01-26-21'],\n",
       " ['0.0', '-76.65514', '-11.23284', '2019-01-27-04'],\n",
       " ['0.0', '-71.49511', '-16.31086', '2019-02-26-19'],\n",
       " ['0.0', '-78.3079', '-8.00419', '2019-02-24-21'],\n",
       " ['0.0', '-76.35011', '-11.57056', '2019-02-27-14'],\n",
       " ['0.0', '-69.578', '-10.95589', '2019-02-26-08'],\n",
       " ['0.0', '-75.62756', '-5.07975', '2019-02-28-06'],\n",
       " ['0.0', '-80.39898', '-3.97972', '2019-02-28-21'],\n",
       " ['1.0', '-70.75028', '-9.75028', '2019-01-31-11'],\n",
       " ['0.0', '-78.18667', '-5.88667', '2019-02-28-02'],\n",
       " ['0.0', '-78.86721', '-6.57489', '2019-01-28-03'],\n",
       " ['0.0', '-76.25961', '-11.73447', '2019-02-25-05'],\n",
       " ['0.0', '-72.06289', '-14.52328', '2019-01-27-22'],\n",
       " ['0.0', '-75.58928', '-13.84358', '2019-01-30-11'],\n",
       " ['0.0', '-71.65124', '-15.18255', '2019-01-28-15'],\n",
       " ['0.0', '-76.02778', '-9.32', '2019-02-27-11'],\n",
       " ['0.0', '-74.51953', '-8.36225', '2019-01-30-11'],\n",
       " ['0.0', '-74.95588', '-14.632', '2019-02-26-18'],\n",
       " ['1.3', '-70.28831', '-17.11413', '2019-01-27-16'],\n",
       " ['0.0', '-79.68203', '-6.06692', '2019-02-27-08'],\n",
       " ['0.0', '-78.38012', '-7.79636', '2019-01-29-14'],\n",
       " ['0.0', '-74.51953', '-8.36225', '2019-01-29-15'],\n",
       " ['0.2', '-69.94863', '-17.3692', '2019-02-23-19'],\n",
       " ['0.0', '-76.76952', '-11.38887', '2019-01-27-04'],\n",
       " ['0.9', '-76.5909', '-9.87895', '2019-02-25-21'],\n",
       " ['16.8', '-80.39788', '-4.48047', '2019-02-23-19'],\n",
       " ['0.0', '-78.69759', '-5.45077', '2019-02-26-02'],\n",
       " ['0.0', '-72.89331', '-15.21134', '2019-01-30-14'],\n",
       " ['0.0', '-76.00207', '-9.29601', '2019-02-24-11'],\n",
       " ['0.0', '-74.51953', '-8.36225', '2019-01-30-12'],\n",
       " ['0.0', '-75.40066', '-13.78826', '2019-02-28-01'],\n",
       " ['0.0', '-78.78432', '-6.54049', '2019-02-23-16'],\n",
       " ['0.0', '-80.17296', '-5.1128', '2019-02-26-18'],\n",
       " ['0.1', '-77.45365', '-9.72919', '2019-02-26-11'],\n",
       " ['0.0', '-77.36577', '-9.16249', '2019-01-26-09'],\n",
       " ['0.1', '-77.45365', '-9.72919', '2019-02-25-13'],\n",
       " ['0.0', '-79.18869', '-7.75031', '2019-02-27-17'],\n",
       " ['0.0', '-78.40242', '-7.67042', '2019-01-30-03'],\n",
       " ['0.0', '-80.45689', '-3.7687', '2019-01-31-05'],\n",
       " ['0.0', '-78.27598', '-5.81968', '2019-02-28-00'],\n",
       " ['0.0', '-76.66664', '-11.92', '2019-01-26-18'],\n",
       " ['0.0', '-78.67588', '-6.55405', '2019-01-29-16'],\n",
       " ['0.0', '-72.29832', '-13.98923', '2019-02-27-18'],\n",
       " ['0.5', '-78.99989', '-5.14422', '2019-01-27-07'],\n",
       " ['0.0', '-76.77167', '-11.57833', '2019-01-28-13'],\n",
       " ['0.0', '-76.02778', '-9.32', '2019-01-30-01'],\n",
       " ['0.0', '-69.62672', '-17.57994', '2019-02-25-21']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02c47c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comienzo de la epoca 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shounen\\AppData\\Local\\Temp/ipykernel_528412/2147333938.py:61: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  field = ds.variables['CMI'][:].data.astype(np.float32)/100.0\n",
      "C:\\Users\\Shounen\\AppData\\Local\\Temp/ipykernel_528412/2147333938.py:64: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  lon = ds.variables['longitude'][:].data\n",
      "C:\\Users\\Shounen\\AppData\\Local\\Temp/ipykernel_528412/2147333938.py:65: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  lat = ds.variables['latitude'][:].data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 1s 169ms/step - loss: 0.1684 - mean_squared_error: 0.9835 - val_loss: 0.5227 - val_mean_squared_error: 7.1509\n",
      "\n",
      "Comienzo de la epoca 1\n",
      "5/5 [==============================] - 1s 226ms/step - loss: 0.1685 - mean_squared_error: 0.9835 - val_loss: 0.5229 - val_mean_squared_error: 7.1507\n"
     ]
    }
   ],
   "source": [
    "#Entrenamos con TODO el dataset\n",
    "entrenamiento(modelo,datasetList,path_base,[1,1],products, batch,epocas=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a71ef85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comienzo de la epoca 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shounen\\AppData\\Local\\Temp/ipykernel_528412/2147333938.py:61: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  field = ds.variables['CMI'][:].data.astype(np.float32)/100.0\n",
      "C:\\Users\\Shounen\\AppData\\Local\\Temp/ipykernel_528412/2147333938.py:64: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  lon = ds.variables['longitude'][:].data\n",
      "C:\\Users\\Shounen\\AppData\\Local\\Temp/ipykernel_528412/2147333938.py:65: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  lat = ds.variables['latitude'][:].data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"sequential\" (type Sequential).\n\nInput 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=1. Full shape received: (64,)\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(64,), dtype=float64)\n  • training=True\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_528412/2711233641.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Entrenamos con BATCH\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mentrenamiento\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelo\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdatasetList\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpath_base\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mproducts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mepocas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_528412/978927533.py\u001b[0m in \u001b[0;36mentrenamiento\u001b[1;34m(model, datasetList, path_base, margen, products, batch_size, epocas)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m                 \u001b[1;31m#Entrenamos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m                 \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train_batch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_train_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_acc_metric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[1;31m#Imprimir log\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_528412/4253910378.py\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(x, y, model, optimizer, loss_fn, train_acc_metric)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_acc_metric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Test\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Test\\lib\\site-packages\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    226\u001b[0m       \u001b[0mndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m         raise ValueError(f'Input {input_index} of layer \"{layer_name}\" '\n\u001b[0m\u001b[0;32m    229\u001b[0m                          \u001b[1;34m'is incompatible with the layer: '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m                          \u001b[1;34mf'expected min_ndim={spec.min_ndim}, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"sequential\" (type Sequential).\n\nInput 0 of layer \"conv2d\" is incompatible with the layer: expected min_ndim=4, found ndim=1. Full shape received: (64,)\n\nCall arguments received:\n  • inputs=tf.Tensor(shape=(64,), dtype=float64)\n  • training=True\n  • mask=None"
     ]
    }
   ],
   "source": [
    "#Entrenamos con BATCH\n",
    "entrenamiento(modelo,datasetList,path_base,[1,1],products, 64 ,epocas=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb6fb71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 3, 3)\n",
      "[[[  1  -1 -11]\n",
      "  [  2  -2 -22]\n",
      "  [  3  -3 -33]]\n",
      "\n",
      " [[  4  -4 -44]\n",
      "  [  5  -5 -55]\n",
      "  [  6  -6 -66]]\n",
      "\n",
      " [[  7  -7 -77]\n",
      "  [  8  -8 -88]\n",
      "  [  9  -9 -99]]]\n"
     ]
    }
   ],
   "source": [
    "arr1 = np.array([[1, 2, 3],[4,5,6],[7,8,9]])\n",
    "\n",
    "arr2 = np.array([[-1, -2, -3],[-4,-5,-6],[-7,-8,-9]])\n",
    "\n",
    "arr3 = np.array([[-11, -22, -33],[-44,-55,-66],[-77,-88,-99]])\n",
    "\n",
    "L = [arr1,arr2,arr3]\n",
    "\n",
    "arr = np.dstack(L)\n",
    "\n",
    "#arr = np.array([arr1,arr2])\n",
    "#arr = np.reshape(arr,(3,3,2))\n",
    "print(arr.shape)\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6656ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a42d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc29e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
